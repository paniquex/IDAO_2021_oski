{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WjN5wgl2GXxM",
    "outputId": "6af4fd4e-23cc-4a0a-e8a0-92acba09c7aa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paniquex/anaconda3/envs/kaggle/lib/python3.7/site-packages/ipykernel_launcher.py:56: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "/home/paniquex/anaconda3/envs/kaggle/lib/python3.7/site-packages/ipykernel_launcher.py:181: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "2.7562: 100%|██████████| 140/140 [00:41<00:00,  3.36it/s]\n",
      "  0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:05<00:00, 12.62it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY STOPPING COUNTER: 1/5\n",
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.62it/s]\n",
      "1.1947: 100%|██████████| 140/140 [00:38<00:00,  3.60it/s]\n",
      "  0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:05<00:00, 13.09it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY STOPPING COUNTER: 1/5\n",
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.63it/s]\n",
      "0.9171: 100%|██████████| 140/140 [00:38<00:00,  3.62it/s]\n",
      "  0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:05<00:00, 12.99it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY STOPPING COUNTER: 1/5\n",
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.61it/s]\n",
      "0.6689: 100%|██████████| 140/140 [00:38<00:00,  3.65it/s]\n",
      "  0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:05<00:00, 13.58it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY STOPPING COUNTER: 1/5\n",
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.71it/s]\n",
      "0.5761: 100%|██████████| 140/140 [00:38<00:00,  3.63it/s]\n",
      "  0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:05<00:00, 13.40it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY STOPPING COUNTER: 1/5\n",
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
      "0.4975: 100%|██████████| 140/140 [00:39<00:00,  3.58it/s]\n",
      "  0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:05<00:00, 13.48it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY STOPPING COUNTER: 2/5\n",
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n",
      "0.5077: 100%|██████████| 140/140 [00:39<00:00,  3.58it/s]\n",
      "  0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:05<00:00, 13.83it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY STOPPING COUNTER: 1/5\n",
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.55it/s]\n",
      "0.4414: 100%|██████████| 140/140 [00:39<00:00,  3.56it/s]\n",
      "  0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:05<00:00, 13.15it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY STOPPING COUNTER: 2/5\n",
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.62it/s]\n",
      "0.3884: 100%|██████████| 140/140 [00:38<00:00,  3.62it/s]\n",
      "  0%|          | 0/71 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71/71 [00:05<00:00, 13.94it/s]\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EARLY STOPPING COUNTER: 1/5\n",
      "Predict test without augmentations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "0.3827:  72%|███████▏  | 101/140 [00:28<00:10,  3.72it/s]"
     ]
    }
   ],
   "source": [
    "COLAB = False\n",
    "USE_PRIVATE_AUGMENTED = True\n",
    "\n",
    "if COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/gdrive\")\n",
    "\n",
    "if COLAB:\n",
    "    !unzip gdrive/MyDrive/idao_data/IDAO_2021_oski.zip\n",
    "  \n",
    "if COLAB:\n",
    "    !pip install transformers\n",
    "    !pip install timm\n",
    "    !pip install albumentations==0.4.6\n",
    "\n",
    "import sys\n",
    "\n",
    "if COLAB:\n",
    "    sys.path.append(\"IDAO_2021_oski/src\")\n",
    "else:\n",
    "    sys.path.append(\"../src\")\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import albumentations\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from datasets import SimpleDataset\n",
    "from models import Wrapper, MixUp\n",
    "from pipeline_utils import training, pseudolabeling\n",
    "from models import ENCODER_PARAMS\n",
    "\n",
    "if COLAB:\n",
    "    PATH_TO_CFG = \"IDAO_2021_oski/config/main.yml\"\n",
    "    !pip install --upgrade --force-reinstall --no-deps albumentations\n",
    "else:\n",
    "    PATH_TO_CFG = \"../config/main.yml\"\n",
    "with open(PATH_TO_CFG, \"r\") as file:\n",
    "    config = yaml.load(file)\n",
    "\n",
    "DATA_ROOT = config[\"paths\"][\"data_path\"]\n",
    "\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "fix_seed(config[\"seed\"])\n",
    "device_ids = [str(id) for id in config[\"device_ids\"]]\n",
    "ids = \",\".join(device_ids)\n",
    "ids = '0'\n",
    "DEVICE = torch.device(f\"cuda:{ids}\")\n",
    "\n",
    "if COLAB:\n",
    "    DATA_ROOT = \"IDAO_2021_oski/data/\"\n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_ROOT, config[\"paths\"][\"train_file\"]), index_col=0)\n",
    "\n",
    "val_private = pd.read_csv(os.path.join(DATA_ROOT, config[\"paths\"][\"val_file\"]), index_col=0)\n",
    "\n",
    "if USE_PRIVATE_AUGMENTED:\n",
    "    augmented_private = pd.read_csv(os.path.join(DATA_ROOT, config[\"paths\"][\"widen_file\"]), index_col=0)\n",
    "    mask = ~augmented_private[\"file_path\"].str.contains(\"train/\")\n",
    "    augmented_private = augmented_private[mask].reset_index()\n",
    "\n",
    "if COLAB:\n",
    "    train[\"file_path\"] = train[\"file_path\"].str.replace(\"../data/\", \"IDAO_2021_oski/data/\")\n",
    "    val_private[\"file_path\"] = val_private[\"file_path\"].str.replace(\"../data/\", \"IDAO_2021_oski/data/\")\n",
    "    if USE_PRIVATE_AUGMENTED:\n",
    "        augmented_private[\"file_path\"] = augmented_private[\"file_path\"].str.replace(\"../data/\", \"IDAO_2021_oski/data/\")\n",
    "        augmented_private.drop(index=0, inplace=True)\n",
    "\n",
    "test = pd.read_csv(os.path.join(DATA_ROOT, config[\"paths\"][\"test_file\"]), index_col=0)\n",
    "if COLAB:\n",
    "    test[\"file_path\"] = test[\"file_path\"].str.replace(\"../data/\", \"IDAO_2021_oski/data/\")\n",
    "\n",
    "def focal_loss(input, target, focus=2.0, raw=False):\n",
    "\n",
    "    if raw:\n",
    "        input = torch.sigmoid(input)\n",
    "\n",
    "    eps = 1e-7\n",
    "\n",
    "    prob_true = input * target + (1 - input) * (1 - target)\n",
    "    prob_true = torch.clamp(prob_true, eps, 1-eps)\n",
    "    modulating_factor = (1.0 - prob_true).pow(focus)\n",
    "\n",
    "    return (-modulating_factor * prob_true.log()).mean()\n",
    "\n",
    "\n",
    "\n",
    "class AngularPenaltySMLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, loss_type=\"cosface\", eps=1e-7, s=None, m=None):\n",
    "        '''\n",
    "        Angular Penalty Softmax Loss\n",
    "        Three 'loss_types' available: ['arcface', 'sphereface', 'cosface']\n",
    "        These losses are described in the following papers: \n",
    "        \n",
    "        ArcFace: https://arxiv.org/abs/1801.07698\n",
    "        SphereFace: https://arxiv.org/abs/1704.08063\n",
    "        CosFace/Ad Margin: https://arxiv.org/abs/1801.05599\n",
    "        '''\n",
    "        super(AngularPenaltySMLoss, self).__init__()\n",
    "        loss_type = loss_type.lower()\n",
    "        assert loss_type in  ['arcface', 'sphereface', 'cosface']\n",
    "        if loss_type == 'arcface':\n",
    "            self.s = 64.0 if not s else s\n",
    "            self.m = 0.5 if not m else m\n",
    "        if loss_type == 'sphereface':\n",
    "            self.s = 64.0 if not s else s\n",
    "            self.m = 1.35 if not m else m\n",
    "        if loss_type == 'cosface':\n",
    "            self.s = 30.0 if not s else s\n",
    "            self.m = 0.4 if not m else m\n",
    "        self.loss_type = loss_type\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        '''\n",
    "        input shape (N, in_features)\n",
    "        '''\n",
    "        assert len(x) == len(labels)\n",
    "        assert torch.min(labels) >= 0\n",
    "        assert torch.max(labels) < self.out_features\n",
    "        \n",
    "        for W in self.fc.parameters():\n",
    "            W = F.normalize(W, p=2, dim=1)\n",
    "\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "#         print(x.shape)\n",
    "        wf = self.fc(x)\n",
    "        if self.loss_type == 'cosface':\n",
    "            numerator = self.s * (torch.diagonal(wf.transpose(0, 1)[labels]) - self.m)\n",
    "        if self.loss_type == 'arcface':\n",
    "            numerator = self.s * torch.cos(torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1.+self.eps, 1-self.eps)) + self.m)\n",
    "        if self.loss_type == 'sphereface':\n",
    "            numerator = self.s * torch.cos(self.m * torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1.+self.eps, 1-self.eps)))\n",
    "        excl = torch.cat([torch.cat((wf[i, :y], wf[i, y+1:])).unsqueeze(0) for i, y in enumerate(labels)], dim=0)\n",
    "        denominator = torch.exp(numerator) + torch.sum(torch.exp(self.s * excl), dim=1)\n",
    "        L = numerator - torch.log(denominator)\n",
    "        return -torch.mean(L), wf\n",
    "\n",
    "MODELS_NAMES = [\"resnest14d_1e-4_joint_BCE_L1_with_private_augmented\", \"resnest50d_4s2x40d_1e-3_joint_BCE_L1_with_private_augmented_old_center_crop=150\", \n",
    "                \"resnest50d_4s2x40d_1e-4_joint_BCE_L1_with_private_augmented\", \"tf_efficientnet_b0_ns_1e-4_joint_BCE_L1_with_private_augmented\", \n",
    "                \"tf_efficientnet_b3_ns_1e-4_joint_BCE_L1_with_private_augmented\"]\n",
    "if COLAB:\n",
    "    CONFIG_ROOT = \"IDAO_2021_oski/config/\"\n",
    "    PREFIX = \"IDAO_2021_oski/\"\n",
    "else:\n",
    "    CONFIG_ROOT = \"../config/\"\n",
    "    PREFIX = \"../\"\n",
    "\n",
    "for model_name in MODELS_NAMES:\n",
    "    PATH_TO_CFG = os.path.join(CONFIG_ROOT, model_name + \"_config.yaml\")\n",
    "    with open(PATH_TO_CFG, \"r\") as file:\n",
    "        model_config = yaml.load(file)\n",
    "    le = LabelEncoder() \n",
    "\n",
    "    mask_NR = (train[\"0\"] == \"NR\") & ((train[\"1\"] == 1) | (train[\"1\"] == 6) | (train[\"1\"] == 20))\n",
    "    mask_ER = (train[\"0\"] == \"ER\") & ((train[\"1\"] == 3) | (train[\"1\"] == 10) | (train[\"1\"] == 30))\n",
    "    train = train[mask_NR | mask_ER]\n",
    "    if USE_PRIVATE_AUGMENTED:\n",
    "        train = train.append(augmented_private)\n",
    "    train.index = pd.RangeIndex(0, len(train.index))\n",
    "    if model_config[\"general\"][\"task_type\"] == \"regression\":\n",
    "        train[\"target\"] = train[\"1\"]\n",
    "        val_private[\"target\"] = val_private[\"1\"]\n",
    "    elif model_config[\"general\"][\"task_type\"] == \"classification\":\n",
    "        train[\"target\"] = le.fit_transform(train[\"target\"])\n",
    "    #     val_private[\"target\"] = le.fit_transform(val_private)\n",
    "    elif model_config[\"general\"][\"task_type\"] == \"joint\":\n",
    "        train[\"target_regression\"] = train[\"1\"]\n",
    "        train[\"target_classification\"] = le.fit_transform(train[\"0\"])\n",
    "        train[\"target\"] = train[\"target_regression\"].astype(str) + \"_\" + train[\"target_classification\"].astype(str)\n",
    "        \n",
    "        val_private[\"target_regression\"] = val_private[\"1\"]\n",
    "        val_private[\"target_classification\"] = le.fit_transform(val_private[\"0\"])\n",
    "\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=model_config[\"training\"][\"n_folds\"], shuffle=True,\n",
    "                            random_state=config[\"seed\"])\n",
    "    for fold, (t_idx, v_idx) in enumerate(kfold.split(train, train[\"target\"])):\n",
    "        train.loc[v_idx, \"kfold\"] = fold\n",
    "\n",
    "\n",
    "        \n",
    "    train.to_csv(os.path.join(DATA_ROOT, \"train\", \"train_folds.csv\"))\n",
    "\n",
    "    transforms_train = albumentations.Compose([\n",
    "        ColorJitter (brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5),\n",
    "        ShiftScaleRotate(shift_limit=0.05, scale_limit=0.05, rotate_limit=15, p=0.5),\n",
    "        CenterCrop(height=400,\n",
    "                  width=400),\n",
    "        Resize(*model_config[\"preprocessing\"][\"img_size\"]),\n",
    "        Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "\n",
    "    transforms_val = albumentations.Compose([\n",
    "        CenterCrop(height=400,\n",
    "                  width=400),\n",
    "        Resize(*model_config[\"preprocessing\"][\"img_size\"]),\n",
    "        Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    EPOCHS = model_config[\"training\"][\"n_epochs\"]\n",
    "\n",
    "\n",
    "    criterion_aam = None\n",
    "    if model_config[\"general\"][\"task_type\"] == \"classification\":\n",
    "        if model_config[\"training\"][\"loss\"][\"clf\"] == \"FOCAL\":\n",
    "            criterion = focal_loss\n",
    "        elif model_config[\"training\"][\"loss\"][\"clf\"] == \"AAM\":\n",
    "            criterion = \"AAM\"\n",
    "            criterion_aam = AngularPenaltySMLoss\n",
    "        elif model_config[\"training\"][\"loss\"][\"clf\"] == \"BCE\":\n",
    "            criterion = nn.BCELoss()\n",
    "    elif model_config[\"general\"][\"task_type\"] == \"regression\":\n",
    "        if model_config[\"training\"][\"loss\"][\"reg\"] == \"L1\":\n",
    "            criterion = nn.L1Loss()\n",
    "        elif model_config[\"training\"][\"loss\"][\"reg\"] == \"L2\":\n",
    "            criterion = nn.MSELoss()\n",
    "    elif model_config[\"training\"][\"loss\"] == \"L1\":\n",
    "        criterion = nn.L1Loss()\n",
    "    elif model_config[\"general\"][\"task_type\"] == \"joint\":\n",
    "        criterion = {}\n",
    "        if model_config[\"training\"][\"loss\"][\"clf\"] == \"FOCAL\":\n",
    "            criterion[\"clf\"] = focal_loss\n",
    "        elif model_config[\"training\"][\"loss\"][\"clf\"] == \"AAM\":\n",
    "            criterion[\"clf\"] = \"AAM\"\n",
    "            criterion_aam = AngularPenaltySMLoss\n",
    "        elif model_config[\"training\"][\"loss\"][\"clf\"] == \"BCE\":\n",
    "            criterion[\"clf\"] = nn.BCELoss()\n",
    "        if model_config[\"training\"][\"loss\"][\"reg\"] == \"L1\":\n",
    "            criterion[\"reg\"] = nn.L1Loss()\n",
    "        elif model_config[\"training\"][\"loss\"][\"reg\"] == \"L2\":\n",
    "            criterion[\"reg\"] = nn.MSELoss()\n",
    "\n",
    "    try:\n",
    "        shutil.rmtree(os.path.join(PREFIX, model_config[\"general\"][\"out_path\"]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        os.mkdir(os.path.join(PREFIX, model_config[\"general\"][\"out_path\"]))\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "\n",
    "    if model_config[\"general\"][\"task_type\"] == \"regression\":\n",
    "        model_config[\"general\"][\"classes_num\"] = 1\n",
    "    elif model_config[\"general\"][\"task_type\"] == \"joint\":\n",
    "        model_config[\"general\"][\"classes_num\"] = 2\n",
    "        \n",
    "        \n",
    "    samples2preds_all = {}\n",
    "    samples2trues_all = {}\n",
    "\n",
    "    models = []\n",
    "    for i in range(model_config[\"training\"][\"n_folds\"]):\n",
    "        model_name = model_config[\"general\"][\"model_name\"]\n",
    "        model = None\n",
    "        model = ENCODER_PARAMS[model_name][\"init_op\"]()\n",
    "        model = Wrapper(model, feat_module=None, classes_num=model_config[\"general\"][\"classes_num\"],\n",
    "                        model_name=model_name,\n",
    "                        spec_augmenter=None, \n",
    "                        mixup_module=None,\n",
    "                        task_type=model_config[\"general\"][\"task_type\"],\n",
    "                        activation_func=model_config[\"training\"][\"activation_func\"],\n",
    "                        criterion_aam=criterion_aam)\n",
    "        model.to(DEVICE)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=model_config[\"training\"][\"lr\"])\n",
    "        train_dataset = SimpleDataset(df=train[train[\"kfold\"] != fold], mode=\"train\",\n",
    "                                      transform=transforms_train, classes_num=model_config[\"general\"][\"classes_num\"],\n",
    "                                      task_type=model_config[\"general\"][\"task_type\"])\n",
    "\n",
    "        val_dataset = SimpleDataset(df=train[train[\"kfold\"] == fold], mode=\"val\",\n",
    "                                    transform=transforms_val, classes_num=model_config[\"general\"][\"classes_num\"],\n",
    "                                    task_type=model_config[\"general\"][\"task_type\"])\n",
    "        val_private_dataset = SimpleDataset(df=val_private, mode=\"val\",\n",
    "                                            transform=transforms_val, classes_num=model_config[\"general\"][\"classes_num\"],\n",
    "                                            task_type=model_config[\"general\"][\"task_type\"])\n",
    "        \n",
    "        train_dataloader = DataLoader(train_dataset,\n",
    "                                      **model_config[\"training\"][\"dataloader\"])\n",
    "        val_dataloader = DataLoader(val_dataset,\n",
    "                                    **model_config[\"validation\"][\"dataloader\"])\n",
    "        val_private_dataloader = DataLoader(val_private_dataset,\n",
    "                                            **model_config[\"validation\"][\"dataloader\"])\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                          T_max=(model_config[\"training\"][\"n_epochs\"] - model_config[\"training\"][\"n_epochs_flat\"])  * len(train_dataloader),\n",
    "                                                          eta_min=1e-8)    \n",
    "        samples2preds, samples2trues, model = training(EPOCHS=EPOCHS, model=model,\n",
    "                                                train_dataloader=train_dataloader, \n",
    "                                                val_dataloaders_dct={\"val_dataloader\": val_dataloader,\n",
    "                                                                      \"val_private_dataloader\": val_private_dataloader},\n",
    "                                                DEVICE=DEVICE, criterion=criterion,\n",
    "                                                optimizer=optimizer, scheduler=scheduler,\n",
    "                                                config=model_config, fold=i,\n",
    "                                                task_type=model_config[\"general\"][\"task_type\"], CONFIG_PATH=PATH_TO_CFG, prefix=PREFIX)\n",
    "        models.append(model)\n",
    "        samples2preds_all.update(samples2preds)\n",
    "        samples2trues_all.update(samples2trues)\n",
    "\n",
    "    samples2preds_all = {}\n",
    "    samples2trues_all = {}\n",
    "    LR = model_config[\"training\"][\"lr\"]\n",
    "    flag_LR = True\n",
    "\n",
    "    for j in range(model_config[\"pseudo\"][\"iter\"]):\n",
    "        with torch.no_grad():\n",
    "            train, test = pseudolabeling(models, train, test, model_config, DEVICE, transforms_val)\n",
    "            private = (train[\"type\"].values == \"private\").sum()\n",
    "            public = (train[\"type\"].values == \"public\").sum()\n",
    "            print(\"Pseudo labeling epoch\", j)\n",
    "            print(\"Private ratio\", private / (public + private)) \n",
    "            print(\"Public ratio\", public / (public + private))\n",
    "            if flag_LR:\n",
    "                LR *= model_config[\"pseudo\"][\"lr_coef\"]\n",
    "                flag_LR = False\n",
    "        \n",
    "        for i in range(model_config[\"training\"][\"n_folds\"]):\n",
    "            model = models[i]\n",
    "            model.to(DEVICE)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "            train_dataset = SimpleDataset(df=train[train[\"kfold\"] != fold], mode=\"train\",\n",
    "                                          transform=transforms_train, classes_num=model_config[\"general\"][\"classes_num\"],\n",
    "                                          task_type=model_config[\"general\"][\"task_type\"])\n",
    "\n",
    "            val_dataset = SimpleDataset(df=train[train[\"kfold\"] == fold], mode=\"val\",\n",
    "                                        transform=transforms_val, classes_num=model_config[\"general\"][\"classes_num\"],\n",
    "                                        task_type=model_config[\"general\"][\"task_type\"])\n",
    "            val_private_dataset = SimpleDataset(df=val_private, mode=\"val\",\n",
    "                                                transform=transforms_val, classes_num=model_config[\"general\"][\"classes_num\"],\n",
    "                                                task_type=model_config[\"general\"][\"task_type\"])\n",
    "            \n",
    "            train_dataloader = DataLoader(train_dataset,\n",
    "                                          **model_config[\"training\"][\"dataloader\"])\n",
    "            val_dataloader = DataLoader(val_dataset,\n",
    "                                        **model_config[\"validation\"][\"dataloader\"])\n",
    "            val_private_dataloader = DataLoader(val_private_dataset,\n",
    "                                                **model_config[\"validation\"][\"dataloader\"])\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                              T_max=(model_config[\"pseudo\"][\"n_epochs\"] - model_config[\"pseudo\"][\"n_epochs_flat\"])  * len(train_dataloader),\n",
    "                                                              eta_min=1e-8)    \n",
    "            samples2preds, samples2trues, model = training(EPOCHS=model_config[\"pseudo\"][\"n_epochs\"], model=model,\n",
    "                                                    train_dataloader=train_dataloader, \n",
    "                                                    val_dataloaders_dct={\"val_dataloader\": val_dataloader,\n",
    "                                                                          \"val_private_dataloader\": val_private_dataloader},\n",
    "                                                    DEVICE=DEVICE, criterion=criterion,\n",
    "                                                    optimizer=optimizer, scheduler=scheduler,\n",
    "                                                    config=model_config, fold=i, pseudo_iter=j+1,\n",
    "                                                    task_type=model_config[\"general\"][\"task_type\"], CONFIG_PATH=PATH_TO_CFG)\n",
    "            models[i] = model\n",
    "            samples2preds_all.update(samples2preds)\n",
    "            samples2trues_all.update(samples2trues)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "collab_training_pipeline_pseudolabeling_one_cell.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
