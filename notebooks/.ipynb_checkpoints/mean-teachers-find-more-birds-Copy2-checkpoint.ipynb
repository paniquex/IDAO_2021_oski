{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "I wanted to share something that worked pretty well for me early on in this competition. The idea comes from a [2018 paper](https://arxiv.org/pdf/1703.01780.pdf) titled *Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results* by Antti Tarvainen and Harri Valpola. \n",
    "\n",
    "### Mean Teacher\n",
    "Biefly, the idea is to use two models. A student model with weights trained the standard way, using backprop. And a teacher model with weights that are an exponential moving average of the student's weights. The teacher is the *mean* of the student \\*ba dum tss\\*. The student is then trained using two different losses, a standard classification loss and a consistency loss that penalizes student predictions that deviate from the teaher's. \n",
    "\n",
    "![](https://raw.githubusercontent.com/CuriousAI/mean-teacher/master/mean_teacher.png)\n",
    "\n",
    "Mean teachers are useful in a semi-supervised context where we have both labeled and unlabeled samples. The consistency loss on the unlabeled samples acts as a form of regularization and helps the model generalize better. As an added bonus the final teacher model is a temporal ensemble which tends to perform better than the results at the end of a single epoch. \n",
    "\n",
    "### Missing Labels\n",
    "As a few others have pointed out, there are a lot of missing labels. If we were to randomly sample a segment from the training data, we might consider it completely unlabeled rather than rely on the provided labels. We'll train our mean teacher model(s) on two classes of data, carefully selected positive samples and randomly selected unlabeled samples. The classification loss won't apply to the unlabeled samples. \n",
    "\n",
    "![](https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F4704212%2F9ca088bb386abf7114543c019c1d8a5f%2Ffig.png?generation=1609892974092435&alt=media)\n",
    "\n",
    "*Thanks to [shinmura0](https://www.kaggle.com/shinmurashinmura) for the great visualization!*\n",
    "\n",
    "### Results\n",
    "For me, mean teacher worked a good bit better than baseline models with similar configurations. \n",
    "\n",
    "|                                         | Baseline | Mean Teacher |\n",
    "|-----------------------------------------|----------|--------------|\n",
    "| Well Tuned, 5 fold, from my local setup | 0.847        | **0.865**            |\n",
    "| Single fold Expt1 on Kaggle                   | 0.592**        | **0.786**            |\n",
    "| Single fold Expt2 on Kaggle                   | 0.826        | **0.830**            |\n",
    "| 5 Fold on Kaggle***                        | 0.844        | **0.857**           |\n",
    "\n",
    "\\*\\* I might have accidentally sabatoged this run.\n",
    "\n",
    "\\*\\*\\* There was a major bug in v21 of the notebook where the consistence_ramp was set to 1000 which means it was just normal / non-mean-teacher training. Setting consisteny_ramp to 6 and using the mean teacher, we get an improvement of 0.13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "!pip -q install --upgrade pip\n",
    "!pip -q install timm\n",
    "!pip -q install torchlibrosa\n",
    "!pip -q install audiomentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "import audiomentations as A\n",
    "import os, time, librosa, random\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from timm.models import resnet34d, resnest26d, resnest50d\n",
    "from timm.models.efficientnet import tf_efficientnet_b4_ns, tf_efficientnet_b3_ns, \\\n",
    "    tf_efficientnet_b5_ns, tf_efficientnet_b2_ns, tf_efficientnet_b6_ns, tf_efficientnet_b7_ns, tf_efficientnet_b0_ns, tf_efficientnet_b1_ns\n",
    "from torchlibrosa.stft import Spectrogram, LogmelFilterBank\n",
    "from torchlibrosa.augmentation import SpecAugmentation\n",
    "from tqdm import tqdm\n",
    "import soundfile as sf\n",
    "from contextlib import nullcontext\n",
    "\n",
    "\n",
    "from ranger import Ranger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixUp(nn.Module):\n",
    "    def __init__(self, prob=0.33, alpha=8, mixup_mode=\"basic\"):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.prob = prob\n",
    "        self.mixup_mode = mixup_mode\n",
    "        \n",
    "    def forward(self, waveforms, labels):\n",
    "        inds = np.arange(waveforms.shape[0])\n",
    "        new_inds = inds.copy()\n",
    "        np.random.shuffle(new_inds)\n",
    "        aug_count = int(inds[inds != new_inds].shape[0] * self.prob)\n",
    "        to_augment = np.random.choice(inds[inds != new_inds], aug_count, replace=False)\n",
    "        betas = torch.tensor(np.random.beta(self.alpha, self.alpha, size=aug_count),\n",
    "                             dtype=torch.float).unsqueeze(1).to(waveforms.device)\n",
    "        # new_inds = torch.tensor(new_inds)\n",
    "        # to_augment = torch.tensor(to_augment)\n",
    "        waveforms[to_augment] = betas * waveforms[to_augment] + (1 - betas) * waveforms[new_inds][to_augment]\n",
    "        if self.mixup_mode == \"basic\":\n",
    "            labels[to_augment] = betas * labels[to_augment] + (1 - betas) * labels[new_inds][to_augment]\n",
    "        elif self.mixup_mode == \"or\":\n",
    "            labels[to_augment] = torch.clamp_max(labels[to_augment] + labels[new_inds][to_augment], max=1.)\n",
    "        return waveforms, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config\n",
    "We'll start by setting up some global config variable that we'll access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Global Vars\n",
    "NO_LABEL = -1\n",
    "NUM_CLASSES = 24\n",
    "\n",
    "LOSS_TYPE = \"LSEP\"\n",
    "\n",
    "class config:\n",
    "    seed = 42\n",
    "    device = \"cuda:1\"\n",
    "    \n",
    "    train_tp_csv = '/media/paniquex/samsung_2tb/rfcx_kaggle/rfcx-species-audio-detection/train_tp.csv'\n",
    "    test_csv = '/media/paniquex/samsung_2tb/rfcx_kaggle/rfcx-species-audio-detection/sample_submission.csv'\n",
    "    save_path = '../experiments/mean_teacher_mels=224_BCE_eff_b0_adamw_period=3_val=3_encoder_percent_unlabeled=1.0_consistency_weight=50_consistency_rampup=6_ema_decay=0.995'\n",
    "    \n",
    "    encoder = tf_efficientnet_b0_ns\n",
    "    encoder_features = 1280\n",
    "    \n",
    "    percent_unlabeled = 1.0\n",
    "    consistency_weight = 100.0\n",
    "    consistency_rampup = 6\n",
    "    \n",
    "    ema_decay = 0.995\n",
    "    positive_weight = 2.0\n",
    "    \n",
    "    lr = 1e-3\n",
    "    epochs = 25\n",
    "    batch_size = 16\n",
    "    num_workers = 8\n",
    "    train_5_folds = True\n",
    "    \n",
    "    period = 3 # 6 second clips\n",
    "    period_val = 3\n",
    "\n",
    "    \n",
    "    step = 1\n",
    "    model_params = {\n",
    "        'sample_rate': 48000,\n",
    "        'window_size': 1024,\n",
    "        'hop_size': 345,\n",
    "        'mel_bins': 224,\n",
    "        'fmin': 20,\n",
    "        'fmax': 48000 // 2,\n",
    "        'classes_num': NUM_CLASSES,\n",
    "        'mixup_module': None\n",
    "    }\n",
    "    \n",
    "    augmenter = A.Compose([\n",
    "        A.AddGaussianNoise(p=0.33, max_amplitude=0.02),\n",
    "        A.AddGaussianSNR(p=0.33),\n",
    "        A.FrequencyMask(min_frequency_band=0.01,  max_frequency_band=0.25, p=0.33),\n",
    "        A.TimeMask(min_band_part=0.01, max_band_part=0.25, p=0.33),\n",
    "        A.Gain(p=0.33)\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    shutil.rmtree(config.save_path)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(config.save_path)\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [],
   "source": [
    "## Utils - Not much interesting going on here.\n",
    "\n",
    "def get_n_fold_df(csv_path, folds=5):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    df_group = df.groupby(\"recording_id\")[[\"species_id\"]].first().reset_index()\n",
    "    df_group = df_group.sample(frac=1, random_state=config.seed).reset_index(drop=True)\n",
    "    df_group.loc[:, 'fold'] = -1\n",
    "\n",
    "    X = df_group[\"recording_id\"].values\n",
    "    y = df_group[\"species_id\"].values\n",
    "\n",
    "    kfold = StratifiedKFold(n_splits=folds, random_state=config.seed)\n",
    "    for fold, (t_idx, v_idx) in enumerate(kfold.split(X, y)):\n",
    "        df_group.loc[v_idx, \"fold\"] = fold\n",
    "\n",
    "    return df.merge(df_group[['recording_id', 'fold']], on=\"recording_id\", how=\"left\")\n",
    "    \n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def sigmoid_rampup(current, rampup_length):\n",
    "    \"\"\"Exponential rampup from https://arxiv.org/abs/1610.02242\"\"\"\n",
    "    if rampup_length == 0:\n",
    "        return 1.0\n",
    "    else:\n",
    "        current = np.clip(current, 0.0, rampup_length)\n",
    "        phase = 1.0 - current / rampup_length\n",
    "        return float(np.exp(-5.0 * phase * phase))\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "class MetricMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.y_true = []\n",
    "        self.y_pred = []\n",
    "\n",
    "    def update(self, y_true, y_pred):\n",
    "        try:\n",
    "            self.y_true.extend(y_true.detach().cpu().numpy().tolist())\n",
    "            self.y_pred.extend(torch.sigmoid(y_pred).cpu().detach().numpy().tolist())\n",
    "        except:\n",
    "            print(\"UPDATE FAILURE\")\n",
    "\n",
    "    def update_list(self, y_true, y_pred):\n",
    "        self.y_true.extend(y_true)\n",
    "        self.y_pred.extend(y_pred)\n",
    "\n",
    "    @property\n",
    "    def avg(self):\n",
    "        score_class, weight = lwlrap(np.array(self.y_true), np.array(self.y_pred))\n",
    "        self.score = (score_class * weight).sum()\n",
    "\n",
    "        return self.score\n",
    "    \n",
    "\n",
    "def interpolate(x: torch.Tensor, ratio: int):\n",
    "    \"\"\"Interpolate data in time domain. This is used to compensate the\n",
    "    resolution reduction in downsampling of a CNN.\n",
    "\n",
    "    Args:\n",
    "      x: (batch_size, time_steps, classes_num)\n",
    "      ratio: int, ratio to interpolate\n",
    "    Returns:\n",
    "      upsampled: (batch_size, time_steps * ratio, classes_num)\n",
    "    \"\"\"\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "def _one_sample_positive_class_precisions(scores, truth):\n",
    "    num_classes = scores.shape[0]\n",
    "    pos_class_indices = np.flatnonzero(truth > 0)\n",
    "\n",
    "    if not len(pos_class_indices):\n",
    "        return pos_class_indices, np.zeros(0)\n",
    "\n",
    "    retrieved_classes = np.argsort(scores)[::-1]\n",
    "\n",
    "    class_rankings = np.zeros(num_classes, dtype=np.int)\n",
    "    class_rankings[retrieved_classes] = range(num_classes)\n",
    "\n",
    "    retrieved_class_true = np.zeros(num_classes, dtype=np.bool)\n",
    "    retrieved_class_true[class_rankings[pos_class_indices]] = True\n",
    "\n",
    "    retrieved_cumulative_hits = np.cumsum(retrieved_class_true)\n",
    "\n",
    "    precision_at_hits = (\n",
    "            retrieved_cumulative_hits[class_rankings[pos_class_indices]] /\n",
    "            (1 + class_rankings[pos_class_indices].astype(np.float)))\n",
    "    return pos_class_indices, precision_at_hits\n",
    "\n",
    "\n",
    "def lwlrap(truth, scores):\n",
    "    assert truth.shape == scores.shape\n",
    "    num_samples, num_classes = scores.shape\n",
    "    precisions_for_samples_by_classes = np.zeros((num_samples, num_classes))\n",
    "    for sample_num in range(num_samples):\n",
    "        pos_class_indices, precision_at_hits = _one_sample_positive_class_precisions(scores[sample_num, :],\n",
    "                                                                                     truth[sample_num, :])\n",
    "        precisions_for_samples_by_classes[sample_num, pos_class_indices] = precision_at_hits\n",
    "\n",
    "    labels_per_class = np.sum(truth > 0, axis=0)\n",
    "    weight_per_class = labels_per_class / float(np.sum(labels_per_class))\n",
    "\n",
    "    per_class_lwlrap = (np.sum(precisions_for_samples_by_classes, axis=0) /\n",
    "                        np.maximum(1, labels_per_class))\n",
    "    return per_class_lwlrap, weight_per_class\n",
    "\n",
    "\n",
    "def pretty_print_metrics(fold, epoch, optimizer, train_loss_metrics, val_loss_metrics):\n",
    "    print(f\"\"\"\n",
    "    {time.ctime()} \\n\n",
    "    Fold:{fold}, Epoch:{epoch}, LR:{optimizer.param_groups[0]['lr']:.7}, Cons. Weight: {train_loss_metrics['consistency_weight']}\\n\n",
    "    --------------------------------------------------------\n",
    "    Metric:              Train    |   Val\n",
    "    --------------------------------------------------------\n",
    "    Loss:                {train_loss_metrics['loss']:0.4f}   |   {val_loss_metrics['loss']:0.4f}\\n\n",
    "    LWLRAP:              {train_loss_metrics['lwlrap']:0.4f}   |   {val_loss_metrics['lwlrap']:0.4f}\\n\n",
    "    Class Loss:          {train_loss_metrics['class_loss']:0.4f}   |   {val_loss_metrics['class_loss']:0.4f}\\n\n",
    "    Consistency Loss:    {train_loss_metrics['consistency_loss']:0.4f}   |   {val_loss_metrics['consistency_loss']:0.4f}\\n\n",
    "    --------------------------------------------------------\\n\n",
    "    \"\"\")\n",
    "    \n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, df, data_path, period=10, step=1):\n",
    "        self.data_path = data_path\n",
    "        self.period = period\n",
    "        self.step = step\n",
    "        self.recording_ids = list(df[\"recording_id\"].unique())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.recording_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        recording_id = self.recording_ids[idx]\n",
    "\n",
    "        y, sr = sf.read(f\"{self.data_path}/{recording_id}.flac\")\n",
    "\n",
    "        len_y = len(y)\n",
    "        effective_length = sr * self.period\n",
    "        effective_step = sr * self.step\n",
    "\n",
    "        y_ = []\n",
    "        i = 0\n",
    "        while i+effective_length <= len_y:\n",
    "            y__ = y[i:i + effective_length]\n",
    "\n",
    "            y_.append(y__)\n",
    "            i = i + effective_step\n",
    "\n",
    "        y = np.stack(y_)\n",
    "\n",
    "        label = np.zeros(NUM_CLASSES, dtype='f')\n",
    "\n",
    "        return {\n",
    "            \"waveform\": y,\n",
    "            \"target\": torch.tensor(label, dtype=torch.float),\n",
    "            \"id\": recording_id\n",
    "        }\n",
    "\n",
    "\n",
    "def predict_on_test(model, test_loader):\n",
    "    model.eval()\n",
    "    pred_list = []\n",
    "    id_list = []\n",
    "    with torch.no_grad():\n",
    "        t = tqdm(test_loader)\n",
    "        for i, sample in enumerate(t):\n",
    "            input = sample[\"waveform\"].to(config.device)\n",
    "            bs, seq, w = input.shape\n",
    "            input = input.reshape(bs * seq, w)\n",
    "            id = sample[\"id\"]\n",
    "            output, _ = model(input)\n",
    "            output = output.reshape(bs, seq, -1)\n",
    "            output, _ = torch.max(output, dim=1)\n",
    "            \n",
    "            output = output.cpu().detach().numpy().tolist()\n",
    "            pred_list.extend(output)\n",
    "            id_list.extend(id)\n",
    "\n",
    "    return pred_list, id_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "The model should look pretty familiar if you're using [SED](https://arxiv.org/abs/1912.04761). (Huge thanks to [Hidehisa Arai](https://www.kaggle.com/hidehisaarai1213) and their [SED Notebook](https://www.kaggle.com/hidehisaarai1213/introduction-to-sound-event-detection)!) You could use any model you'd like here. There's just one small tweak we need to make for our mean teacher setup. We need to \"detach\" the teacher's parameters so they aren't updated by the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class SEDAudioClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, sample_rate, window_size, hop_size, \n",
    "                 mel_bins, fmin, fmax, classes_num, mixup_module=None):\n",
    "        super().__init__()\n",
    "        self.interpolate_ratio = 32\n",
    "\n",
    "        self.spectrogram_extractor = Spectrogram(n_fft=window_size, \n",
    "                                                 hop_length=hop_size,\n",
    "                                                 win_length=window_size, \n",
    "                                                 window='hann', center=True,\n",
    "                                                 pad_mode='reflect', \n",
    "                                                 freeze_parameters=True)\n",
    "        self.logmel_extractor = LogmelFilterBank(sr=sample_rate, n_fft=window_size,\n",
    "                                                 n_mels=mel_bins, fmin=fmin, \n",
    "                                                 fmax=fmax, ref=1.0, \n",
    "                                                 amin=1e-10, top_db=None, \n",
    "                                                 freeze_parameters=True)\n",
    "\n",
    "        self.batch_norm = nn.BatchNorm2d(mel_bins)\n",
    "        self.encoder = partial(config.encoder, pretrained=True, in_chans=1)()\n",
    "        self.fc = nn.Linear(config.encoder_features, \n",
    "                            config.encoder_features, bias=True)\n",
    "        self.att_head = AttBlockV2(config.encoder_features, classes_num)\n",
    "        self.avg_pool = nn.modules.pooling.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        self.mixup_module = mixup_module\n",
    "        \n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.batch_norm)\n",
    "        init_layer(self.fc)\n",
    "        self.att_head.init_weights()\n",
    "\n",
    "    def forward(self, input, labels=None, spec_aug=False, return_encoding=False):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if self.mixup_module and labels is not None:\n",
    "                input, labels = self.mixup_module(input, labels)\n",
    "        \n",
    "        x = self.spectrogram_extractor(input.float())\n",
    "        x = self.logmel_extractor(x)\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.batch_norm(x)\n",
    "        x = x.transpose(1, 3)\n",
    "\n",
    "        x = self.encoder.forward_features(x)\n",
    "        x = torch.mean(x, dim=3)\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "#         print(x.shape)\n",
    "        x = F.relu_(self.fc(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_head(x)\n",
    "        logit = torch.sum(norm_att * self.att_head.cla(x), dim=2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        framewise_output = interpolate(segmentwise_output, self.interpolate_ratio)\n",
    "        if labels is not None:\n",
    "            return clipwise_output, framewise_output, logit, labels\n",
    "        else:\n",
    "            return clipwise_output, framewise_output, labels\n",
    "\n",
    "\n",
    "def get_model(is_mean_teacher=False):\n",
    "    model = SEDAudioClassifier(**config.model_params)\n",
    "    model = model.to(config.device)\n",
    "    \n",
    "    # Detach params for Exponential Moving Average Model (aka the Mean Teacher).\n",
    "    # We'll manually update these params instead of using backprop.\n",
    "    if is_mean_teacher:\n",
    "        for param in model.parameters():\n",
    "            param.detach_()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Function\n",
    "The loss function has 2 components:\n",
    "\n",
    "1. A classification loss that only applies to labeled samples.\n",
    "2. A consistency loss that applies to all samples. \n",
    "\n",
    "For the consistency loss we'll use the mean square error between the student and teacher predictions. We'll slowly ramp up the influence of the consistency loss since we don't want bad, early predictions having too much influence. \n",
    "\n",
    "Notice that we're weighting the positive samples for the classification loss. This is because we know the positives are correct while we're less sure about the negatives due to the missing labels issue. I found that this works better in practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedPANNsLoss(nn.Module):\n",
    "    def __init__(self, pos_weight, weights=[1, 0.5]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.normal_loss = nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
    "\n",
    "        self.bce = nn.BCEWithLogitsLoss(reduction='none', pos_weight=pos_weight)\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, input, framewise_output, target):\n",
    "        input_ = input\n",
    "        target = target.float()\n",
    "\n",
    "        clipwise_output_with_max, _ = framewise_output.max(dim=1)\n",
    "\n",
    "        normal_loss = self.normal_loss(input_, target)\n",
    "        auxiliary_loss = self.bce(clipwise_output_with_max, target)\n",
    "\n",
    "        return self.weights[0] * normal_loss + self.weights[1] * auxiliary_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_mse_loss(input_logits, target_logits):\n",
    "    assert input_logits.size() == target_logits.size()\n",
    "    input_softmax = torch.sigmoid(input_logits)\n",
    "    target_softmax = torch.sigmoid(target_logits)\n",
    "    num_classes = input_logits.size()[1]\n",
    "    return F.mse_loss(input_softmax, target_softmax, size_average=False\n",
    "                     ) / num_classes\n",
    "\n",
    "def lsep_loss_stable(input, target, average=True):\n",
    "\n",
    "    n = input.size(0)\n",
    "\n",
    "    differences = input.unsqueeze(1) - input.unsqueeze(2)\n",
    "    where_lower = (target.unsqueeze(1) < target.unsqueeze(2)).float()\n",
    "\n",
    "    differences = differences.view(n, -1)\n",
    "    where_lower = where_lower.view(n, -1)\n",
    "\n",
    "    max_difference, index = torch.max(differences, dim=1, keepdim=True)\n",
    "    differences = differences - max_difference\n",
    "    exps = differences.exp() * where_lower\n",
    "\n",
    "    lsep = max_difference + torch.log(torch.exp(-max_difference) + exps.sum(-1))\n",
    "\n",
    "    if average:\n",
    "        return lsep.mean()\n",
    "    else:\n",
    "        return lsep\n",
    "\n",
    "\n",
    "class MeanTeacherLoss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.positive_weight = torch.ones(\n",
    "            NUM_CLASSES).to(config.device) * config.positive_weight\n",
    "        self.class_criterion = nn.BCEWithLogitsLoss(\n",
    "            reduction='none', pos_weight=self.positive_weight)\n",
    "        self.consistency_criterion = sigmoid_mse_loss\n",
    "\n",
    "    def make_safe(self, pred):\n",
    "        pred = torch.where(torch.isnan(pred), torch.zeros_like(pred), pred)\n",
    "        return torch.where(torch.isinf(pred), torch.zeros_like(pred), pred)\n",
    "        \n",
    "    def get_consistency_weight(self, epoch):\n",
    "        # Consistency ramp-up from https://arxiv.org/abs/1610.02242\n",
    "        return config.consistency_weight * sigmoid_rampup(\n",
    "            epoch, config.consistency_rampup)\n",
    "    \n",
    "    def forward(self, student_pred, teacher_pred, target, classif_weights, epoch):\n",
    "        student_pred = self.make_safe(student_pred)\n",
    "        teacher_pred = self.make_safe(teacher_pred).detach().data\n",
    "\n",
    "        batch_size = len(target)\n",
    "        labeled_batch_size = target.ne(NO_LABEL).all(axis=1).sum().item() + 1e-3\n",
    "\n",
    "        student_classif, student_consistency = student_pred, student_pred\n",
    "        student_class_loss = (self.class_criterion(\n",
    "            student_classif, target) * classif_weights / labeled_batch_size).sum()\n",
    "\n",
    "        consistency_weights = self.get_consistency_weight(epoch)\n",
    "        consistency_loss = consistency_weights * self.consistency_criterion(\n",
    "            student_consistency, teacher_pred) / batch_size\n",
    "        loss = student_class_loss + consistency_loss\n",
    "        return loss, student_class_loss, consistency_loss, consistency_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader\n",
    "The data loader produces two types of samples:\n",
    "\n",
    "1. Labeled samples with the audio centered in the clip.\n",
    "2. Random unlabeled clips without labels selected from files with at least one true positive label.\n",
    "\n",
    "Each sample contains 2 different inputs, one for the student and one for the teacher. Different augmentations are applied to each input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanTeacherDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, df, transforms, period=5, \n",
    "                 data_path=\"/media/paniquex/samsung_2tb/rfcx_kaggle/rfcx-species-audio-detection/train\", \n",
    "                 val=False, percent_unlabeled=0.0):\n",
    "        self.period = period\n",
    "        self.transforms = transforms\n",
    "        self.data_path = data_path\n",
    "        self.val = val\n",
    "        self.percent_unlabeled = percent_unlabeled\n",
    "\n",
    "        dfgby = df.groupby(\"recording_id\").agg(lambda x: list(x)).reset_index()\n",
    "        self.recording_ids = dfgby[\"recording_id\"].values\n",
    "        self.species_ids = dfgby[\"species_id\"].values\n",
    "        self.t_mins = dfgby[\"t_min\"].values\n",
    "        self.t_maxs = dfgby[\"t_max\"].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(len(self.recording_ids) * (1 + self.percent_unlabeled))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx >= len(self.recording_ids):\n",
    "            audio, label, rec_id, sr = self.get_unlabeled_item(idx)\n",
    "            # For unlabeled samples, we zero out the classification loss.\n",
    "            classif_weights = np.zeros(NUM_CLASSES, dtype='f')\n",
    "        else:\n",
    "            audio, label, rec_id, sr = self.get_labeled_item(idx)\n",
    "            classif_weights = np.ones(NUM_CLASSES, dtype='f')\n",
    "\n",
    "        audio_teacher = np.copy(audio)\n",
    "\n",
    "        # The 2 samples fed to the 2 models have should have different augmentations.\n",
    "        audio = self.transforms(samples=audio, sample_rate=sr)\n",
    "        audio_teacher = self.transforms(samples=audio_teacher, sample_rate=sr)\n",
    "        # assert (audio != audio_teacher).any()\n",
    "        \n",
    "        return {\n",
    "            \"waveform\": audio,\n",
    "            \"teacher_waveform\": audio_teacher,\n",
    "            \"target\": torch.tensor(label, dtype=torch.float),\n",
    "            \"classification_weights\": classif_weights,\n",
    "            \"id\": rec_id\n",
    "        }\n",
    "\n",
    "    def get_labeled_item(self, idx):\n",
    "        recording_id = self.recording_ids[idx]\n",
    "        species_id = self.species_ids[idx]\n",
    "        t_min, t_max = self.t_mins[idx], self.t_maxs[idx]\n",
    "\n",
    "        rec, sr = sf.read(f\"{self.data_path}/{recording_id}.flac\")\n",
    "\n",
    "        len_rec = len(rec)\n",
    "        effective_length = sr * self.period\n",
    "        rint = np.random.randint(len(t_min))\n",
    "        tmin, tmax = round(sr * t_min[rint]), round(sr * t_max[rint])\n",
    "        dur = tmax - tmin\n",
    "        min_dur = min(dur, round(sr * self.period))\n",
    "\n",
    "        center = round((tmin + tmax) / 2)\n",
    "        rand_start = center - effective_length + max(min_dur - dur//2, 0)\n",
    "        if rand_start < 0:\n",
    "            rand_start = 0\n",
    "        rand_end = center - max(min_dur - dur//2, 0)\n",
    "        start = np.random.randint(rand_start, rand_end)\n",
    "        rec = rec[start:start + effective_length]\n",
    "        if len(rec) < effective_length:\n",
    "            new_rec = np.zeros(effective_length, dtype=rec.dtype)\n",
    "            start1 = np.random.randint(effective_length - len(rec))\n",
    "            new_rec[start1:start1 + len(rec)] = rec\n",
    "            rec = new_rec.astype(np.float32)\n",
    "        else:\n",
    "            rec = rec.astype(np.float32)\n",
    "\n",
    "        start_time = start / sr\n",
    "        end_time = (start + effective_length) / sr\n",
    "\n",
    "        label = np.zeros(NUM_CLASSES, dtype='f')\n",
    "\n",
    "        for i in range(len(t_min)):\n",
    "            if (t_min[i] >= start_time) & (t_max[i] <= end_time):\n",
    "                label[species_id[i]] = 1\n",
    "            elif start_time <= ((t_min[i] + t_max[i]) / 2) <= end_time:\n",
    "                label[species_id[i]] = 1\n",
    "\n",
    "        return rec, label, recording_id, sr\n",
    "\n",
    "    def get_unlabeled_item(self, idx, random_sample=False):\n",
    "        real_idx = idx - len(self.recording_ids)\n",
    "        # We want our validation set to be fixed.\n",
    "        if self.val:\n",
    "            rec_id = self.recording_ids[real_idx]\n",
    "        else:\n",
    "            rec_id = random.sample(list(self.recording_ids), 1)[0]\n",
    "\n",
    "        rec, sr = sf.read(f\"{self.data_path}/{rec_id}.flac\")\n",
    "        effective_length = int(sr * self.period)\n",
    "        max_end = len(rec) - effective_length\n",
    "        if self.val:\n",
    "            # Fixed start for validation. Probaably a better way to do this.\n",
    "            start = int(idx * 16963 % max_end)\n",
    "        else:\n",
    "            start = np.random.randint(0, max_end)\n",
    "        rec = rec[start:(start+effective_length)]\n",
    "        rec = rec.astype(np.float32)\n",
    "\n",
    "        label = np.ones(NUM_CLASSES, dtype='f') * NO_LABEL\n",
    "\n",
    "        return rec, label, rec_id, sr\n",
    "\n",
    "    \n",
    "def get_data_loader(df, is_val=False):\n",
    "    if is_val:\n",
    "        period = config.period_val\n",
    "    else:\n",
    "        period = config.period\n",
    "    dataset = MeanTeacherDataset(\n",
    "        df=df,\n",
    "        transforms=config.augmenter,\n",
    "        period=period,\n",
    "        percent_unlabeled=config.percent_unlabeled\n",
    "    )\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        shuffle=not is_val,\n",
    "        drop_last=not is_val,\n",
    "        num_workers=config.num_workers\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "At the end of each training step we update the teacher weights by averaging in the latest student weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update teacher to be exponential moving average of student params.\n",
    "def update_teacher_params(student, teacher, alpha, global_step):\n",
    "    # Use the true average until the exponential average is more correct\n",
    "    alpha = min(1 - 1 / (global_step + 1), alpha)\n",
    "    for ema_param, param in zip(teacher.parameters(), student.parameters()):\n",
    "        ema_param.data.mul_(alpha).add_(1 - alpha, param.data)\n",
    "\n",
    "\n",
    "def train_one_epoch(student, mean_teacher, loader, \n",
    "                    criterion, optimizer, scheduler, epoch, is_val=False):\n",
    "    global_step = 0\n",
    "    losses = AverageMeter()\n",
    "    consistency_loss_avg = AverageMeter()\n",
    "    class_loss_avg = AverageMeter()\n",
    "    comp_metric = MetricMeter()\n",
    "    \n",
    "    if is_val:\n",
    "        student.eval()\n",
    "        mean_teacher.eval()\n",
    "        context = torch.no_grad()\n",
    "    else:\n",
    "        student.train()\n",
    "        mean_teacher.train()\n",
    "        context = nullcontext()\n",
    "    \n",
    "    with context:\n",
    "        t = tqdm(loader)\n",
    "        for i, sample in enumerate(t):\n",
    "            student_input = sample['waveform'].to(config.device)\n",
    "            teacher_input = sample['teacher_waveform'].to(config.device)\n",
    "            target = sample['target'].to(config.device)\n",
    "            classif_weights = sample['classification_weights'].to(config.device)\n",
    "            batch_size = len(target)\n",
    "            \n",
    "            if student.mixup_module:\n",
    "                student_pred, framewise_output, logit, target  = student(student_input, labels=target)\n",
    "            else:\n",
    "                student_pred, framewise_output, logit, target  = student(student_input, labels=target)\n",
    "#             if teacher.mixup_module:\n",
    "#                 teacher_pred, _, target  = mean_teacher(teacher_input, labels=target)\n",
    "#             else:\n",
    "            teacher_pred, _, _ = mean_teacher(teacher_input)\n",
    "\n",
    "            loss, class_loss, consistency_loss, consistency_weight = criterion(\n",
    "                student_pred, teacher_pred, target, classif_weights, epoch)\n",
    "\n",
    "            if not is_val:\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                update_teacher_params(student, mean_teacher, \n",
    "                                      config.ema_decay, global_step)\n",
    "\n",
    "                scheduler.step()\n",
    "\n",
    "            comp_metric.update(target, student_pred)\n",
    "            losses.update(loss.item(), batch_size)\n",
    "            consistency_loss_avg.update(consistency_loss.item(), batch_size)\n",
    "            class_loss_avg.update(class_loss.item(), batch_size)\n",
    "            global_step += 1\n",
    "\n",
    "            t.set_description(f\"Epoch:{epoch} - Loss:{losses.avg:0.4f}\")\n",
    "        t.close()\n",
    "    return {'lwlrap':comp_metric.avg, \n",
    "            'loss':losses.avg, \n",
    "            'consistency_loss':consistency_loss_avg.avg, \n",
    "            'class_loss':class_loss_avg.avg, \n",
    "            'consistency_weight':consistency_weight}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally putting everything together..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paniquex/anaconda3/envs/kaggle/lib/python3.7/site-packages/sklearn/model_selection/_split.py:297: FutureWarning: Setting a random_state has no effect since shuffle is False. This will raise an error in 0.24. You should leave random_state to its default (None), or set shuffle=True.\n",
      "  FutureWarning\n",
      "/home/paniquex/anaconda3/envs/kaggle/lib/python3.7/site-packages/librosa/filters.py:235: UserWarning: Empty filters detected in mel frequency basis. Some channels will produce empty responses. Try increasing your sampling rate (and fmax) or reducing n_mels.\n",
      "  warnings.warn('Empty filters detected in mel frequency basis. '\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]/home/paniquex/anaconda3/envs/kaggle/lib/python3.7/site-packages/torch/nn/_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n",
      "/home/paniquex/anaconda3/envs/kaggle/lib/python3.7/site-packages/ipykernel_launcher.py:6: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n",
      "  \n",
      "Epoch:0 - Loss:7.2125: 100%|██████████| 113/113 [00:33<00:00,  3.42it/s]\n",
      "Epoch:0 - Loss:6.9789: 100%|██████████| 28/28 [00:07<00:00,  3.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:37:35 2021 \n",
      "\n",
      "    Fold:0, Epoch:0, LR:0.0009960574, Cons. Weight: 0.6737946999085467\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                7.2125   |   6.9789\n",
      "\n",
      "    LWLRAP:              0.2718   |   0.3581\n",
      "\n",
      "    Class Loss:          7.2039   |   6.9631\n",
      "\n",
      "    Consistency Loss:    0.0086   |   0.0159\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from -inf --> 0.3580927799528169\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 - Loss:5.3211: 100%|██████████| 113/113 [00:33<00:00,  3.39it/s]\n",
      "Epoch:1 - Loss:5.1167: 100%|██████████| 28/28 [00:07<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:38:16 2021 \n",
      "\n",
      "    Fold:0, Epoch:1, LR:0.0009842916, Cons. Weight: 3.1047958479329627\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                5.3211   |   5.1167\n",
      "\n",
      "    LWLRAP:              0.4934   |   0.5787\n",
      "\n",
      "    Class Loss:          5.2836   |   5.0357\n",
      "\n",
      "    Consistency Loss:    0.0375   |   0.0810\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.3580927799528169 --> 0.5787351177376753\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 - Loss:4.3920: 100%|██████████| 113/113 [00:33<00:00,  3.36it/s]\n",
      "Epoch:2 - Loss:3.9539: 100%|██████████| 28/28 [00:08<00:00,  3.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:38:59 2021 \n",
      "\n",
      "    Fold:0, Epoch:2, LR:0.0009648882, Cons. Weight: 10.836802322189582\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                4.3920   |   3.9539\n",
      "\n",
      "    LWLRAP:              0.6265   |   0.6711\n",
      "\n",
      "    Class Loss:          4.2485   |   3.7571\n",
      "\n",
      "    Consistency Loss:    0.1435   |   0.1968\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.5787351177376753 --> 0.6710961176745451\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 - Loss:3.5849: 100%|██████████| 113/113 [00:33<00:00,  3.36it/s]\n",
      "Epoch:3 - Loss:3.2903: 100%|██████████| 28/28 [00:10<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:39:43 2021 \n",
      "\n",
      "    Fold:0, Epoch:3, LR:0.0009381533, Cons. Weight: 28.650479686019008\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.5849   |   3.2903\n",
      "\n",
      "    LWLRAP:              0.7328   |   0.7659\n",
      "\n",
      "    Class Loss:          3.2689   |   2.9163\n",
      "\n",
      "    Consistency Loss:    0.3160   |   0.3740\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.6710961176745451 --> 0.7658932644964637\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:4 - Loss:3.3703: 100%|██████████| 113/113 [00:41<00:00,  2.75it/s]\n",
      "Epoch:4 - Loss:3.5135: 100%|██████████| 28/28 [00:10<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:40:35 2021 \n",
      "\n",
      "    Fold:0, Epoch:4, LR:0.0009045085, Cons. Weight: 57.375342073743276\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.3703   |   3.5135\n",
      "\n",
      "    LWLRAP:              0.7860   |   0.7970\n",
      "\n",
      "    Class Loss:          2.7514   |   2.7414\n",
      "\n",
      "    Consistency Loss:    0.6188   |   0.7721\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.7658932644964637 --> 0.7970348556089103\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:5 - Loss:3.1645: 100%|██████████| 113/113 [00:40<00:00,  2.80it/s]\n",
      "Epoch:5 - Loss:3.9545: 100%|██████████| 28/28 [00:09<00:00,  2.83it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:41:26 2021 \n",
      "\n",
      "    Fold:0, Epoch:5, LR:0.0008644843, Cons. Weight: 87.03247258333906\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.1645   |   3.9545\n",
      "\n",
      "    LWLRAP:              0.8138   |   0.7778\n",
      "\n",
      "    Class Loss:          2.4251   |   2.9368\n",
      "\n",
      "    Consistency Loss:    0.7393   |   1.0177\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:6 - Loss:2.9853: 100%|██████████| 113/113 [00:32<00:00,  3.50it/s]\n",
      "Epoch:6 - Loss:3.7239: 100%|██████████| 28/28 [00:06<00:00,  4.04it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:42:05 2021 \n",
      "\n",
      "    Fold:0, Epoch:6, LR:0.000818712, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.9853   |   3.7239\n",
      "\n",
      "    LWLRAP:              0.8462   |   0.8096\n",
      "\n",
      "    Class Loss:          2.1631   |   2.5936\n",
      "\n",
      "    Consistency Loss:    0.8222   |   1.1303\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.7970348556089103 --> 0.8096411346411346\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:7 - Loss:2.7890: 100%|██████████| 113/113 [00:32<00:00,  3.47it/s]\n",
      "Epoch:7 - Loss:4.1507: 100%|██████████| 28/28 [00:07<00:00,  3.97it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:42:45 2021 \n",
      "\n",
      "    Fold:0, Epoch:7, LR:0.0007679134, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.7890   |   4.1507\n",
      "\n",
      "    LWLRAP:              0.8540   |   0.7885\n",
      "\n",
      "    Class Loss:          2.0466   |   3.0474\n",
      "\n",
      "    Consistency Loss:    0.7423   |   1.1033\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:8 - Loss:2.3979: 100%|██████████| 113/113 [00:32<00:00,  3.51it/s]\n",
      "Epoch:8 - Loss:3.3717: 100%|██████████| 28/28 [00:06<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:43:24 2021 \n",
      "\n",
      "    Fold:0, Epoch:8, LR:0.0007128896, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.3979   |   3.3717\n",
      "\n",
      "    LWLRAP:              0.8813   |   0.8308\n",
      "\n",
      "    Class Loss:          1.6483   |   2.3220\n",
      "\n",
      "    Consistency Loss:    0.7496   |   1.0497\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8096411346411346 --> 0.830819762613977\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:9 - Loss:2.4936: 100%|██████████| 113/113 [00:32<00:00,  3.45it/s]\n",
      "Epoch:9 - Loss:3.0343: 100%|██████████| 28/28 [00:08<00:00,  3.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:44:05 2021 \n",
      "\n",
      "    Fold:0, Epoch:9, LR:0.0006545085, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.4936   |   3.0343\n",
      "\n",
      "    LWLRAP:              0.8719   |   0.8513\n",
      "\n",
      "    Class Loss:          1.7612   |   2.1510\n",
      "\n",
      "    Consistency Loss:    0.7324   |   0.8832\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.830819762613977 --> 0.8512654547311502\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:10 - Loss:2.0187: 100%|██████████| 113/113 [00:31<00:00,  3.56it/s]\n",
      "Epoch:10 - Loss:3.2220: 100%|██████████| 28/28 [00:08<00:00,  3.42it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:44:46 2021 \n",
      "\n",
      "    Fold:0, Epoch:10, LR:0.0005936907, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.0187   |   3.2220\n",
      "\n",
      "    LWLRAP:              0.8978   |   0.8312\n",
      "\n",
      "    Class Loss:          1.3915   |   2.3441\n",
      "\n",
      "    Consistency Loss:    0.6272   |   0.8779\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:11 - Loss:2.0570: 100%|██████████| 113/113 [00:32<00:00,  3.47it/s]\n",
      "Epoch:11 - Loss:2.5370: 100%|██████████| 28/28 [00:08<00:00,  3.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:45:26 2021 \n",
      "\n",
      "    Fold:0, Epoch:11, LR:0.0005313953, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.0570   |   2.5370\n",
      "\n",
      "    LWLRAP:              0.9057   |   0.8622\n",
      "\n",
      "    Class Loss:          1.4177   |   1.7671\n",
      "\n",
      "    Consistency Loss:    0.6393   |   0.7700\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8512654547311502 --> 0.8622341286018416\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:12 - Loss:2.0506: 100%|██████████| 113/113 [00:29<00:00,  3.88it/s]\n",
      "Epoch:12 - Loss:3.4243: 100%|██████████| 28/28 [00:05<00:00,  5.16it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:46:01 2021 \n",
      "\n",
      "    Fold:0, Epoch:12, LR:0.0004686047, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.0506   |   3.4243\n",
      "\n",
      "    LWLRAP:              0.9061   |   0.8451\n",
      "\n",
      "    Class Loss:          1.3218   |   2.6487\n",
      "\n",
      "    Consistency Loss:    0.7288   |   0.7756\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:13 - Loss:1.6706: 100%|██████████| 113/113 [00:27<00:00,  4.07it/s]\n",
      "Epoch:13 - Loss:2.9069: 100%|██████████| 28/28 [00:07<00:00,  3.85it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:46:36 2021 \n",
      "\n",
      "    Fold:0, Epoch:13, LR:0.0004063093, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.6706   |   2.9069\n",
      "\n",
      "    LWLRAP:              0.9214   |   0.8435\n",
      "\n",
      "    Class Loss:          1.0798   |   2.1530\n",
      "\n",
      "    Consistency Loss:    0.5908   |   0.7539\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:14 - Loss:1.7090: 100%|██████████| 113/113 [00:31<00:00,  3.59it/s]\n",
      "Epoch:14 - Loss:2.7016: 100%|██████████| 28/28 [00:07<00:00,  3.66it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:47:16 2021 \n",
      "\n",
      "    Fold:0, Epoch:14, LR:0.0003454915, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.7090   |   2.7016\n",
      "\n",
      "    LWLRAP:              0.9248   |   0.8479\n",
      "\n",
      "    Class Loss:          1.0754   |   2.0077\n",
      "\n",
      "    Consistency Loss:    0.6336   |   0.6940\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:15 - Loss:1.4823: 100%|██████████| 113/113 [00:31<00:00,  3.59it/s]\n",
      "Epoch:15 - Loss:2.7012: 100%|██████████| 28/28 [00:07<00:00,  3.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:47:55 2021 \n",
      "\n",
      "    Fold:0, Epoch:15, LR:0.0002871104, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.4823   |   2.7012\n",
      "\n",
      "    LWLRAP:              0.9431   |   0.8660\n",
      "\n",
      "    Class Loss:          0.9055   |   2.0459\n",
      "\n",
      "    Consistency Loss:    0.5768   |   0.6554\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8622341286018416 --> 0.8660185846084775\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:16 - Loss:1.4533: 100%|██████████| 113/113 [00:30<00:00,  3.74it/s]\n",
      "Epoch:16 - Loss:2.8948: 100%|██████████| 28/28 [00:09<00:00,  3.00it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:48:35 2021 \n",
      "\n",
      "    Fold:0, Epoch:16, LR:0.0002320866, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.4533   |   2.8948\n",
      "\n",
      "    LWLRAP:              0.9464   |   0.8377\n",
      "\n",
      "    Class Loss:          0.8933   |   2.3348\n",
      "\n",
      "    Consistency Loss:    0.5600   |   0.5600\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:17 - Loss:1.3446: 100%|██████████| 113/113 [00:31<00:00,  3.64it/s]\n",
      "Epoch:17 - Loss:2.5766: 100%|██████████| 28/28 [00:08<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:49:15 2021 \n",
      "\n",
      "    Fold:0, Epoch:17, LR:0.000181288, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.3446   |   2.5766\n",
      "\n",
      "    LWLRAP:              0.9449   |   0.8782\n",
      "\n",
      "    Class Loss:          0.8102   |   2.0084\n",
      "\n",
      "    Consistency Loss:    0.5345   |   0.5683\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8660185846084775 --> 0.8782107733703619\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:18 - Loss:1.4260: 100%|██████████| 113/113 [00:31<00:00,  3.55it/s]\n",
      "Epoch:18 - Loss:2.8406: 100%|██████████| 28/28 [00:08<00:00,  3.14it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:49:56 2021 \n",
      "\n",
      "    Fold:0, Epoch:18, LR:0.0001355157, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.4260   |   2.8406\n",
      "\n",
      "    LWLRAP:              0.9411   |   0.8763\n",
      "\n",
      "    Class Loss:          0.8304   |   2.1589\n",
      "\n",
      "    Consistency Loss:    0.5956   |   0.6817\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:19 - Loss:1.2613: 100%|██████████| 113/113 [00:30<00:00,  3.70it/s]\n",
      "Epoch:19 - Loss:2.3292: 100%|██████████| 28/28 [00:07<00:00,  3.93it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:50:34 2021 \n",
      "\n",
      "    Fold:0, Epoch:19, LR:9.54915e-05, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.2613   |   2.3292\n",
      "\n",
      "    LWLRAP:              0.9542   |   0.8715\n",
      "\n",
      "    Class Loss:          0.7306   |   1.7498\n",
      "\n",
      "    Consistency Loss:    0.5307   |   0.5794\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:20 - Loss:1.2665: 100%|██████████| 113/113 [00:31<00:00,  3.59it/s]\n",
      "Epoch:20 - Loss:2.3565: 100%|██████████| 28/28 [00:06<00:00,  4.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:51:12 2021 \n",
      "\n",
      "    Fold:0, Epoch:20, LR:6.184666e-05, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.2665   |   2.3565\n",
      "\n",
      "    LWLRAP:              0.9521   |   0.8995\n",
      "\n",
      "    Class Loss:          0.7264   |   1.7333\n",
      "\n",
      "    Consistency Loss:    0.5401   |   0.6233\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8782107733703619 --> 0.8995333429846297\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:21 - Loss:1.1372: 100%|██████████| 113/113 [00:29<00:00,  3.78it/s]\n",
      "Epoch:21 - Loss:2.5627: 100%|██████████| 28/28 [00:07<00:00,  3.97it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:51:49 2021 \n",
      "\n",
      "    Fold:0, Epoch:21, LR:3.511176e-05, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.1372   |   2.5627\n",
      "\n",
      "    LWLRAP:              0.9625   |   0.8661\n",
      "\n",
      "    Class Loss:          0.6489   |   1.9677\n",
      "\n",
      "    Consistency Loss:    0.4883   |   0.5950\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:22 - Loss:1.0896: 100%|██████████| 113/113 [00:28<00:00,  3.92it/s]\n",
      "Epoch:22 - Loss:2.5284: 100%|██████████| 28/28 [00:08<00:00,  3.31it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:52:27 2021 \n",
      "\n",
      "    Fold:0, Epoch:22, LR:1.570842e-05, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.0896   |   2.5284\n",
      "\n",
      "    LWLRAP:              0.9607   |   0.8633\n",
      "\n",
      "    Class Loss:          0.6065   |   2.0084\n",
      "\n",
      "    Consistency Loss:    0.4831   |   0.5200\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:23 - Loss:1.1477: 100%|██████████| 113/113 [00:31<00:00,  3.59it/s]\n",
      "Epoch:23 - Loss:2.5464: 100%|██████████| 28/28 [00:09<00:00,  2.95it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:53:08 2021 \n",
      "\n",
      "    Fold:0, Epoch:23, LR:3.942649e-06, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.1477   |   2.5464\n",
      "\n",
      "    LWLRAP:              0.9640   |   0.8627\n",
      "\n",
      "    Class Loss:          0.6361   |   2.0509\n",
      "\n",
      "    Consistency Loss:    0.5116   |   0.4956\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:24 - Loss:1.1006: 100%|██████████| 113/113 [00:31<00:00,  3.64it/s]\n",
      "Epoch:24 - Loss:2.5788: 100%|██████████| 28/28 [00:09<00:00,  2.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:53:48 2021 \n",
      "\n",
      "    Fold:0, Epoch:24, LR:0.0, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.1006   |   2.5788\n",
      "\n",
      "    LWLRAP:              0.9581   |   0.8608\n",
      "\n",
      "    Class Loss:          0.6337   |   2.1007\n",
      "\n",
      "    Consistency Loss:    0.4668   |   0.4781\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:0 - Loss:7.1442: 100%|██████████| 113/113 [00:33<00:00,  3.42it/s]\n",
      "Epoch:0 - Loss:6.2908: 100%|██████████| 28/28 [00:08<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:54:32 2021 \n",
      "\n",
      "    Fold:1, Epoch:0, LR:0.0009960574, Cons. Weight: 0.6737946999085467\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                7.1442   |   6.2908\n",
      "\n",
      "    LWLRAP:              0.2813   |   0.4218\n",
      "\n",
      "    Class Loss:          7.1358   |   6.2787\n",
      "\n",
      "    Consistency Loss:    0.0084   |   0.0121\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from -inf --> 0.4217978245109793\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 - Loss:5.1010: 100%|██████████| 113/113 [00:39<00:00,  2.84it/s]\n",
      "Epoch:1 - Loss:4.6904: 100%|██████████| 28/28 [00:08<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:55:21 2021 \n",
      "\n",
      "    Fold:1, Epoch:1, LR:0.0009842916, Cons. Weight: 3.1047958479329627\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                5.1010   |   4.6904\n",
      "\n",
      "    LWLRAP:              0.5164   |   0.6029\n",
      "\n",
      "    Class Loss:          5.0613   |   4.6266\n",
      "\n",
      "    Consistency Loss:    0.0397   |   0.0638\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.4217978245109793 --> 0.6029401312803983\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 - Loss:4.1033: 100%|██████████| 113/113 [00:41<00:00,  2.75it/s]\n",
      "Epoch:2 - Loss:4.2760: 100%|██████████| 28/28 [00:05<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:56:09 2021 \n",
      "\n",
      "    Fold:1, Epoch:2, LR:0.0009648882, Cons. Weight: 10.836802322189582\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                4.1033   |   4.2760\n",
      "\n",
      "    LWLRAP:              0.6571   |   0.7006\n",
      "\n",
      "    Class Loss:          3.9511   |   4.0600\n",
      "\n",
      "    Consistency Loss:    0.1522   |   0.2160\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.6029401312803983 --> 0.7006090676704713\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 - Loss:3.7445: 100%|██████████| 113/113 [00:28<00:00,  3.92it/s]\n",
      "Epoch:3 - Loss:3.3603: 100%|██████████| 28/28 [00:08<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:56:46 2021 \n",
      "\n",
      "    Fold:1, Epoch:3, LR:0.0009381533, Cons. Weight: 28.650479686019008\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.7445   |   3.3603\n",
      "\n",
      "    LWLRAP:              0.7151   |   0.7767\n",
      "\n",
      "    Class Loss:          3.3648   |   2.9219\n",
      "\n",
      "    Consistency Loss:    0.3797   |   0.4384\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.7006090676704713 --> 0.7766931699330455\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:4 - Loss:3.9867: 100%|██████████| 113/113 [00:40<00:00,  2.82it/s]\n",
      "Epoch:4 - Loss:4.8566: 100%|██████████| 28/28 [00:09<00:00,  2.80it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:57:37 2021 \n",
      "\n",
      "    Fold:1, Epoch:4, LR:0.0009045085, Cons. Weight: 57.375342073743276\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.9867   |   4.8566\n",
      "\n",
      "    LWLRAP:              0.7326   |   0.7107\n",
      "\n",
      "    Class Loss:          3.3779   |   3.7209\n",
      "\n",
      "    Consistency Loss:    0.6088   |   1.1358\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:5 - Loss:3.5292: 100%|██████████| 113/113 [00:40<00:00,  2.80it/s]\n",
      "Epoch:5 - Loss:3.6756: 100%|██████████| 28/28 [00:09<00:00,  2.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:58:27 2021 \n",
      "\n",
      "    Fold:1, Epoch:5, LR:0.0008644843, Cons. Weight: 87.03247258333906\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.5292   |   3.6756\n",
      "\n",
      "    LWLRAP:              0.7793   |   0.8082\n",
      "\n",
      "    Class Loss:          2.7758   |   2.8096\n",
      "\n",
      "    Consistency Loss:    0.7535   |   0.8660\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.7766931699330455 --> 0.8081652914986248\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:6 - Loss:2.9466: 100%|██████████| 113/113 [00:40<00:00,  2.80it/s]\n",
      "Epoch:6 - Loss:3.6412: 100%|██████████| 28/28 [00:10<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 01:59:18 2021 \n",
      "\n",
      "    Fold:1, Epoch:6, LR:0.000818712, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.9466   |   3.6412\n",
      "\n",
      "    LWLRAP:              0.8307   |   0.8249\n",
      "\n",
      "    Class Loss:          2.2261   |   2.4229\n",
      "\n",
      "    Consistency Loss:    0.7205   |   1.2183\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8081652914986248 --> 0.8249300240929706\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:7 - Loss:2.9373: 100%|██████████| 113/113 [00:40<00:00,  2.82it/s]\n",
      "Epoch:7 - Loss:3.5161: 100%|██████████| 28/28 [00:10<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:00:09 2021 \n",
      "\n",
      "    Fold:1, Epoch:7, LR:0.0007679134, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.9373   |   3.5161\n",
      "\n",
      "    LWLRAP:              0.8457   |   0.8300\n",
      "\n",
      "    Class Loss:          2.1387   |   2.5390\n",
      "\n",
      "    Consistency Loss:    0.7986   |   0.9771\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8249300240929706 --> 0.8299799654710369\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:8 - Loss:2.6109: 100%|██████████| 113/113 [00:40<00:00,  2.77it/s]\n",
      "Epoch:8 - Loss:2.8514: 100%|██████████| 28/28 [00:09<00:00,  2.81it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:01:01 2021 \n",
      "\n",
      "    Fold:1, Epoch:8, LR:0.0007128896, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.6109   |   2.8514\n",
      "\n",
      "    LWLRAP:              0.8579   |   0.8508\n",
      "\n",
      "    Class Loss:          1.8727   |   1.9549\n",
      "\n",
      "    Consistency Loss:    0.7382   |   0.8965\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8299799654710369 --> 0.8508375474083438\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:9 - Loss:2.5114: 100%|██████████| 113/113 [00:41<00:00,  2.72it/s]\n",
      "Epoch:9 - Loss:3.0820: 100%|██████████| 28/28 [00:09<00:00,  2.81it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:01:52 2021 \n",
      "\n",
      "    Fold:1, Epoch:9, LR:0.0006545085, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.5114   |   3.0820\n",
      "\n",
      "    LWLRAP:              0.8686   |   0.8524\n",
      "\n",
      "    Class Loss:          1.8061   |   2.2545\n",
      "\n",
      "    Consistency Loss:    0.7054   |   0.8275\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8508375474083438 --> 0.852354750903858\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:10 - Loss:2.0268: 100%|██████████| 113/113 [00:40<00:00,  2.78it/s]\n",
      "Epoch:10 - Loss:3.3086: 100%|██████████| 28/28 [00:09<00:00,  2.84it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:02:43 2021 \n",
      "\n",
      "    Fold:1, Epoch:10, LR:0.0005936907, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.0268   |   3.3086\n",
      "\n",
      "    LWLRAP:              0.9004   |   0.8259\n",
      "\n",
      "    Class Loss:          1.3626   |   2.5334\n",
      "\n",
      "    Consistency Loss:    0.6642   |   0.7752\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:11 - Loss:2.1331: 100%|██████████| 113/113 [00:40<00:00,  2.82it/s]\n",
      "Epoch:11 - Loss:3.4980: 100%|██████████| 28/28 [00:10<00:00,  2.65it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:03:34 2021 \n",
      "\n",
      "    Fold:1, Epoch:11, LR:0.0005313953, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.1331   |   3.4980\n",
      "\n",
      "    LWLRAP:              0.9033   |   0.8115\n",
      "\n",
      "    Class Loss:          1.4295   |   2.6420\n",
      "\n",
      "    Consistency Loss:    0.7036   |   0.8560\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:12 - Loss:2.0846: 100%|██████████| 113/113 [00:40<00:00,  2.77it/s]\n",
      "Epoch:12 - Loss:2.6899: 100%|██████████| 28/28 [00:09<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:04:24 2021 \n",
      "\n",
      "    Fold:1, Epoch:12, LR:0.0004686047, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.0846   |   2.6899\n",
      "\n",
      "    LWLRAP:              0.8999   |   0.8798\n",
      "\n",
      "    Class Loss:          1.4415   |   1.8065\n",
      "\n",
      "    Consistency Loss:    0.6431   |   0.8834\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.852354750903858 --> 0.8797874779541447\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:13 - Loss:1.9460: 100%|██████████| 113/113 [00:40<00:00,  2.82it/s]\n",
      "Epoch:13 - Loss:3.2518: 100%|██████████| 28/28 [00:08<00:00,  3.31it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:05:13 2021 \n",
      "\n",
      "    Fold:1, Epoch:13, LR:0.0004063093, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.9460   |   3.2518\n",
      "\n",
      "    LWLRAP:              0.9161   |   0.8434\n",
      "\n",
      "    Class Loss:          1.2581   |   2.3867\n",
      "\n",
      "    Consistency Loss:    0.6879   |   0.8652\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:14 - Loss:1.7028: 100%|██████████| 113/113 [00:41<00:00,  2.73it/s]\n",
      "Epoch:14 - Loss:2.6865: 100%|██████████| 28/28 [00:09<00:00,  3.04it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:06:04 2021 \n",
      "\n",
      "    Fold:1, Epoch:14, LR:0.0003454915, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.7028   |   2.6865\n",
      "\n",
      "    LWLRAP:              0.9333   |   0.8742\n",
      "\n",
      "    Class Loss:          1.0563   |   1.7695\n",
      "\n",
      "    Consistency Loss:    0.6465   |   0.9170\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:15 - Loss:1.5837: 100%|██████████| 113/113 [00:41<00:00,  2.74it/s]\n",
      "Epoch:15 - Loss:2.5292: 100%|██████████| 28/28 [00:08<00:00,  3.29it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:06:54 2021 \n",
      "\n",
      "    Fold:1, Epoch:15, LR:0.0002871104, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.5837   |   2.5292\n",
      "\n",
      "    LWLRAP:              0.9340   |   0.8723\n",
      "\n",
      "    Class Loss:          0.9832   |   1.8317\n",
      "\n",
      "    Consistency Loss:    0.6005   |   0.6975\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:16 - Loss:1.6331: 100%|██████████| 113/113 [00:41<00:00,  2.73it/s]\n",
      "Epoch:16 - Loss:2.7766: 100%|██████████| 28/28 [00:08<00:00,  3.28it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:07:44 2021 \n",
      "\n",
      "    Fold:1, Epoch:16, LR:0.0002320866, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.6331   |   2.7766\n",
      "\n",
      "    LWLRAP:              0.9264   |   0.8749\n",
      "\n",
      "    Class Loss:          1.0390   |   2.0267\n",
      "\n",
      "    Consistency Loss:    0.5941   |   0.7499\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:17 - Loss:1.3406: 100%|██████████| 113/113 [00:42<00:00,  2.64it/s]\n",
      "Epoch:17 - Loss:2.7239: 100%|██████████| 28/28 [00:08<00:00,  3.20it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:08:36 2021 \n",
      "\n",
      "    Fold:1, Epoch:17, LR:0.000181288, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.3406   |   2.7239\n",
      "\n",
      "    LWLRAP:              0.9501   |   0.8780\n",
      "\n",
      "    Class Loss:          0.7801   |   2.0762\n",
      "\n",
      "    Consistency Loss:    0.5605   |   0.6477\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:18 - Loss:1.5417: 100%|██████████| 113/113 [00:40<00:00,  2.80it/s]\n",
      "Epoch:18 - Loss:2.9188: 100%|██████████| 28/28 [00:07<00:00,  3.53it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:09:24 2021 \n",
      "\n",
      "    Fold:1, Epoch:18, LR:0.0001355157, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.5417   |   2.9188\n",
      "\n",
      "    LWLRAP:              0.9294   |   0.8649\n",
      "\n",
      "    Class Loss:          0.9691   |   2.2769\n",
      "\n",
      "    Consistency Loss:    0.5726   |   0.6419\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:19 - Loss:1.3957: 100%|██████████| 113/113 [00:41<00:00,  2.71it/s]\n",
      "Epoch:19 - Loss:2.9186: 100%|██████████| 28/28 [00:09<00:00,  3.03it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:10:15 2021 \n",
      "\n",
      "    Fold:1, Epoch:19, LR:9.54915e-05, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.3957   |   2.9186\n",
      "\n",
      "    LWLRAP:              0.9425   |   0.8434\n",
      "\n",
      "    Class Loss:          0.8401   |   2.2723\n",
      "\n",
      "    Consistency Loss:    0.5557   |   0.6463\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:20 - Loss:1.3363: 100%|██████████| 113/113 [00:42<00:00,  2.68it/s]\n",
      "Epoch:20 - Loss:2.7414: 100%|██████████| 28/28 [00:09<00:00,  3.08it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:11:07 2021 \n",
      "\n",
      "    Fold:1, Epoch:20, LR:6.184666e-05, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.3363   |   2.7414\n",
      "\n",
      "    LWLRAP:              0.9452   |   0.8563\n",
      "\n",
      "    Class Loss:          0.8144   |   2.1431\n",
      "\n",
      "    Consistency Loss:    0.5219   |   0.5982\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:21 - Loss:1.0977: 100%|██████████| 113/113 [00:40<00:00,  2.80it/s]\n",
      "Epoch:21 - Loss:2.4883: 100%|██████████| 28/28 [00:09<00:00,  2.89it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:11:57 2021 \n",
      "\n",
      "    Fold:1, Epoch:21, LR:3.511176e-05, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.0977   |   2.4883\n",
      "\n",
      "    LWLRAP:              0.9617   |   0.8732\n",
      "\n",
      "    Class Loss:          0.6356   |   2.0173\n",
      "\n",
      "    Consistency Loss:    0.4621   |   0.4710\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:22 - Loss:1.1462: 100%|██████████| 113/113 [00:40<00:00,  2.82it/s]\n",
      "Epoch:22 - Loss:2.5466: 100%|██████████| 28/28 [00:09<00:00,  2.86it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:12:47 2021 \n",
      "\n",
      "    Fold:1, Epoch:22, LR:1.570842e-05, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.1462   |   2.5466\n",
      "\n",
      "    LWLRAP:              0.9589   |   0.8624\n",
      "\n",
      "    Class Loss:          0.6648   |   1.9896\n",
      "\n",
      "    Consistency Loss:    0.4814   |   0.5570\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:23 - Loss:1.1145: 100%|██████████| 113/113 [00:39<00:00,  2.85it/s]\n",
      "Epoch:23 - Loss:2.7912: 100%|██████████| 28/28 [00:09<00:00,  2.82it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:13:36 2021 \n",
      "\n",
      "    Fold:1, Epoch:23, LR:3.942649e-06, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.1145   |   2.7912\n",
      "\n",
      "    LWLRAP:              0.9607   |   0.8628\n",
      "\n",
      "    Class Loss:          0.6191   |   2.1694\n",
      "\n",
      "    Consistency Loss:    0.4955   |   0.6218\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:24 - Loss:1.1799: 100%|██████████| 113/113 [00:40<00:00,  2.78it/s]\n",
      "Epoch:24 - Loss:2.6903: 100%|██████████| 28/28 [00:09<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:14:27 2021 \n",
      "\n",
      "    Fold:1, Epoch:24, LR:0.0, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                1.1799   |   2.6903\n",
      "\n",
      "    LWLRAP:              0.9601   |   0.8570\n",
      "\n",
      "    Class Loss:          0.6954   |   2.0591\n",
      "\n",
      "    Consistency Loss:    0.4845   |   0.6312\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:0 - Loss:6.8732: 100%|██████████| 113/113 [00:41<00:00,  2.75it/s]\n",
      "Epoch:0 - Loss:4.7903: 100%|██████████| 28/28 [00:10<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:15:20 2021 \n",
      "\n",
      "    Fold:2, Epoch:0, LR:0.0009960574, Cons. Weight: 0.6737946999085467\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                6.8732   |   4.7903\n",
      "\n",
      "    LWLRAP:              0.3127   |   0.5756\n",
      "\n",
      "    Class Loss:          6.8648   |   4.7762\n",
      "\n",
      "    Consistency Loss:    0.0084   |   0.0141\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from -inf --> 0.5755802072464925\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:1 - Loss:5.1329: 100%|██████████| 113/113 [00:41<00:00,  2.72it/s]\n",
      "Epoch:1 - Loss:4.0360: 100%|██████████| 28/28 [00:09<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:16:12 2021 \n",
      "\n",
      "    Fold:2, Epoch:1, LR:0.0009842916, Cons. Weight: 3.1047958479329627\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                5.1329   |   4.0360\n",
      "\n",
      "    LWLRAP:              0.5380   |   0.6934\n",
      "\n",
      "    Class Loss:          5.0859   |   3.9760\n",
      "\n",
      "    Consistency Loss:    0.0470   |   0.0601\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.5755802072464925 --> 0.6933717067685676\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:2 - Loss:3.8671: 100%|██████████| 113/113 [00:40<00:00,  2.77it/s]\n",
      "Epoch:2 - Loss:3.8470: 100%|██████████| 28/28 [00:09<00:00,  2.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:17:03 2021 \n",
      "\n",
      "    Fold:2, Epoch:2, LR:0.0009648882, Cons. Weight: 10.836802322189582\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.8671   |   3.8470\n",
      "\n",
      "    LWLRAP:              0.6907   |   0.7169\n",
      "\n",
      "    Class Loss:          3.7217   |   3.6288\n",
      "\n",
      "    Consistency Loss:    0.1453   |   0.2182\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.6933717067685676 --> 0.7169418624222545\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:3 - Loss:3.5103: 100%|██████████| 113/113 [00:40<00:00,  2.78it/s]\n",
      "Epoch:3 - Loss:3.5704: 100%|██████████| 28/28 [00:10<00:00,  2.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:17:54 2021 \n",
      "\n",
      "    Fold:2, Epoch:3, LR:0.0009381533, Cons. Weight: 28.650479686019008\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.5103   |   3.5704\n",
      "\n",
      "    LWLRAP:              0.7396   |   0.7855\n",
      "\n",
      "    Class Loss:          3.1561   |   3.0850\n",
      "\n",
      "    Consistency Loss:    0.3542   |   0.4854\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.7169418624222545 --> 0.7855398248171283\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:4 - Loss:3.2565: 100%|██████████| 113/113 [00:40<00:00,  2.80it/s]\n",
      "Epoch:4 - Loss:3.7383: 100%|██████████| 28/28 [00:10<00:00,  2.75it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:18:46 2021 \n",
      "\n",
      "    Fold:2, Epoch:4, LR:0.0009045085, Cons. Weight: 57.375342073743276\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.2565   |   3.7383\n",
      "\n",
      "    LWLRAP:              0.7918   |   0.7766\n",
      "\n",
      "    Class Loss:          2.6405   |   2.9391\n",
      "\n",
      "    Consistency Loss:    0.6159   |   0.7992\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:5 - Loss:3.2342: 100%|██████████| 113/113 [00:40<00:00,  2.79it/s]\n",
      "Epoch:5 - Loss:4.0729: 100%|██████████| 28/28 [00:10<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:19:37 2021 \n",
      "\n",
      "    Fold:2, Epoch:5, LR:0.0008644843, Cons. Weight: 87.03247258333906\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.2342   |   4.0729\n",
      "\n",
      "    LWLRAP:              0.8132   |   0.8031\n",
      "\n",
      "    Class Loss:          2.4149   |   2.8897\n",
      "\n",
      "    Consistency Loss:    0.8192   |   1.1831\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.7855398248171283 --> 0.803076863223922\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:6 - Loss:3.1745: 100%|██████████| 113/113 [00:39<00:00,  2.83it/s]\n",
      "Epoch:6 - Loss:3.5580: 100%|██████████| 28/28 [00:10<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:20:28 2021 \n",
      "\n",
      "    Fold:2, Epoch:6, LR:0.000818712, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                3.1745   |   3.5580\n",
      "\n",
      "    LWLRAP:              0.8370   |   0.8296\n",
      "\n",
      "    Class Loss:          2.2956   |   2.4280\n",
      "\n",
      "    Consistency Loss:    0.8789   |   1.1300\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.803076863223922 --> 0.8296290592815256\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:7 - Loss:2.5963: 100%|██████████| 113/113 [00:39<00:00,  2.86it/s]\n",
      "Epoch:7 - Loss:3.5222: 100%|██████████| 28/28 [00:11<00:00,  2.34it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:21:20 2021 \n",
      "\n",
      "    Fold:2, Epoch:7, LR:0.0007679134, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.5963   |   3.5222\n",
      "\n",
      "    LWLRAP:              0.8707   |   0.8236\n",
      "\n",
      "    Class Loss:          1.8103   |   2.5701\n",
      "\n",
      "    Consistency Loss:    0.7860   |   0.9521\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:8 - Loss:2.3824: 100%|██████████| 113/113 [00:38<00:00,  2.91it/s]\n",
      "Epoch:8 - Loss:3.1891: 100%|██████████| 28/28 [00:11<00:00,  2.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:22:10 2021 \n",
      "\n",
      "    Fold:2, Epoch:8, LR:0.0007128896, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.3824   |   3.1891\n",
      "\n",
      "    LWLRAP:              0.8808   |   0.8476\n",
      "\n",
      "    Class Loss:          1.6368   |   2.1537\n",
      "\n",
      "    Consistency Loss:    0.7456   |   1.0354\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n",
      "    LWLRAP Improved from 0.8296290592815256 --> 0.8476250020532532\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:9 - Loss:2.3798: 100%|██████████| 113/113 [00:37<00:00,  3.02it/s]\n",
      "Epoch:9 - Loss:3.4529: 100%|██████████| 28/28 [00:11<00:00,  2.44it/s]\n",
      "  0%|          | 0/113 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Thu Feb 18 02:23:00 2021 \n",
      "\n",
      "    Fold:2, Epoch:9, LR:0.0006545085, Cons. Weight: 100.0\n",
      "\n",
      "    --------------------------------------------------------\n",
      "    Metric:              Train    |   Val\n",
      "    --------------------------------------------------------\n",
      "    Loss:                2.3798   |   3.4529\n",
      "\n",
      "    LWLRAP:              0.8856   |   0.8367\n",
      "\n",
      "    Class Loss:          1.6479   |   2.3742\n",
      "\n",
      "    Consistency Loss:    0.7319   |   1.0788\n",
      "\n",
      "    --------------------------------------------------------\n",
      "\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:10 - Loss:2.0541: 100%|██████████| 113/113 [00:36<00:00,  3.06it/s]\n",
      "Epoch:10 - Loss:3.5370:  29%|██▊       | 8/28 [00:03<00:04,  4.15it/s]"
     ]
    }
   ],
   "source": [
    "def train(df, fold):\n",
    "    train_df = df[df.fold != fold]\n",
    "    val_df = df[df.fold == fold]\n",
    "    train_loader = get_data_loader(train_df)\n",
    "    val_loader = get_data_loader(val_df)\n",
    "\n",
    "    student_model = get_model()\n",
    "    teacher_model = get_model(is_mean_teacher=True)\n",
    "\n",
    "#     optimizer = Ranger(student_model.parameters(),\n",
    "#                lr=config.lr,\n",
    "#                k=4,\n",
    "#                betas=(.9, 0.999), weight_decay=0)\n",
    "    optimizer = torch.optim.AdamW(student_model.parameters(), lr=config.lr)\n",
    "    num_train_steps = int(len(train_loader) * config.epochs)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer, T_max=num_train_steps)\n",
    "    criterion = MeanTeacherLoss()\n",
    "\n",
    "    best_val_metric = -np.inf\n",
    "    val_metrics = []\n",
    "    train_metrics = []\n",
    "    for epoch in range(0, config.epochs):\n",
    "        train_loss_metrics = train_one_epoch(\n",
    "            student_model, teacher_model, train_loader, \n",
    "            criterion, optimizer, scheduler, epoch)\n",
    "        val_loss_metrics = train_one_epoch(\n",
    "            student_model, teacher_model, val_loader, \n",
    "            criterion, optimizer, scheduler, epoch, is_val=True)\n",
    "\n",
    "        train_metrics.append(train_loss_metrics)\n",
    "        val_metrics.append(val_loss_metrics)\n",
    "        pretty_print_metrics(fold, epoch, optimizer, \n",
    "                             train_loss_metrics, val_loss_metrics)\n",
    "        \n",
    "        if val_loss_metrics['lwlrap'] > best_val_metric:\n",
    "            print(f\"    LWLRAP Improved from {best_val_metric} --> {val_loss_metrics['lwlrap']}\\n\")\n",
    "            best_val_metric = val_loss_metrics['lwlrap']\n",
    "            \n",
    "            torch.save(teacher_model.state_dict(), \n",
    "                       os.path.join(config.save_path, f'fold-{fold}_{best_val_metric:.3f}.bin'))\n",
    "    \n",
    "\n",
    "\n",
    "df = get_n_fold_df(config.train_tp_csv)\n",
    "for fold in range(5 if config.train_5_folds else 1):\n",
    "    train(df, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Test Set\n",
    "We'll predict using the teacher model but you could also use the student or a combination of the two. Inference works just like it would for a vanilla baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_df, train_FSLfold):\n",
    "    test_dataset = TestDataset(\n",
    "        df=test_df,\n",
    "        data_path=\"/media/paniquex/samsung_2tb/rfcx_kaggle/rfcx-species-audio-detection/test\",\n",
    "        period=config.period_val,\n",
    "        step=config.step\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "        num_workers=config.num_workers\n",
    "    )\n",
    "    \n",
    "    weights_path = os.path.join(config.save_path, f'fold-{train_fold}.bin')\n",
    "    model = get_model()\n",
    "    model.load_state_dict(torch.load(weights_path, map_location=config.device), strict=False)\n",
    "    \n",
    "    test_pred, ids = predict_on_test(model, test_loader)\n",
    "\n",
    "    # Build Submission File\n",
    "    test_pred_df = pd.DataFrame({\n",
    "        \"recording_id\": test_df.recording_id.values\n",
    "    })\n",
    "    target_cols = test_df.columns[1:].values.tolist()\n",
    "    test_pred_df = test_pred_df.join(pd.DataFrame(np.array(test_pred), \n",
    "                                                  columns=target_cols))\n",
    "    test_pred_df.to_csv(os.path.join(config.save_path, \n",
    "                                     f\"fold-{train_fold}-submission.csv\"), \n",
    "                        index=False)\n",
    "    \n",
    "    \n",
    "test_df = pd.read_csv(config.test_csv)\n",
    "for fold in range(5 if config.train_5_folds else 1):\n",
    "    test(test_df, fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 Fold Ensemble\n",
    "For 5 fold runs, we'll create a single ensemble prediction by simply averaging all of the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble(submission_path):\n",
    "    dfs = [pd.read_csv(os.path.join(\n",
    "        config.save_path, f\"fold-{i}-submission.csv\")) for i in range(5)]\n",
    "    anchor = dfs[0].copy()\n",
    "    cols = anchor.columns[1:]\n",
    "   \n",
    "    for c in cols:\n",
    "        total = 0\n",
    "        for df in dfs:\n",
    "            total += df[c]\n",
    "        anchor[c] = total / len(dfs)\n",
    "    anchor.to_csv(submission_path, index=False)\n",
    "\n",
    "\n",
    "submission_path = os.path.join(config.save_path, f\"submission.csv\")\n",
    "if config.train_5_folds:\n",
    "    ensemble(submission_path)\n",
    "else:\n",
    "    fold0_submission = os.path.join(config.save_path, f\"fold-0-submission.csv\")\n",
    "    os.rename(fold0_submission, submission_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "Thanks for reading! I dropped some unrelated tricks from this and didn't spend much time tuning so there's almost definetely room for improvement.\n",
    "\n",
    "I know it's pretty late in the competition for new notebooks, but considering that there are a few other public notebooks that score higher, I'm hoping this won't cause a significant shakeup. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
