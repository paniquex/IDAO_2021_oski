{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/media/paniquex/samsung_2tb/IDAO_2021_oski/src\")\n",
    "\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "from datasets import SimpleDataset\n",
    "from models import Wrapper, MixUp\n",
    "from pipeline_utils import training\n",
    "from models import ENCODER_PARAMS\n",
    "from ranger import Ranger\n",
    "\n",
    "\n",
    "os.chdir(\"/media/paniquex/samsung_2tb/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paniquex/anaconda3/envs/kaggle/lib/python3.7/site-packages/ipykernel_launcher.py:3: YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe. Please read https://msg.pyyaml.org/load for full details.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "PATH_TO_CFG = \"/media/paniquex/samsung_2tb/IDAO_2021_oski/config/config.yaml\"\n",
    "with open(PATH_TO_CFG, \"r\") as file:\n",
    "    config = yaml.load(file)\n",
    "\n",
    "DATA_ROOT = config[\"general\"][\"data_root\"]\n",
    "\n",
    "def fix_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)  # type: ignore\n",
    "    torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "    torch.backends.cudnn.benchmark = True  # type: ignore\n",
    "\n",
    "fix_seed(config[\"general\"][\"seed\"])\n",
    "device_ids = [str(id) for id in config[\"general\"][\"device_ids\"]]\n",
    "ids = \",\".join(device_ids)\n",
    "DEVICE = torch.device(f\"cuda:{ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/media/paniquex/samsung_2tb/IDAO_2021_oski/data/track_1/idao_dataset'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_ROOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder() \n",
    "\n",
    "train = pd.read_csv(os.path.join(DATA_ROOT, \"train\", \"train.csv\"), index_col=0)\n",
    "\n",
    "\n",
    "mask_NR = (train[\"0\"] == \"NR\") & ((train[\"1\"] == 1) | (train[\"1\"] == 6) | (train[\"1\"] == 20))\n",
    "mask_ER = (train[\"0\"] == \"ER\") & ((train[\"1\"] == 3) | (train[\"1\"] == 10) | (train[\"1\"] == 30))\n",
    "train = train[mask_NR | mask_ER]\n",
    "train.index = pd.RangeIndex(0, len(train.index))\n",
    "if config[\"general\"][\"task_type\"] == \"regression\":\n",
    "    train[\"target\"] = train[\"1\"]\n",
    "elif config[\"general\"][\"task_type\"] == \"classification\":\n",
    "    train[\"target\"] = le.fit_transform(train[\"target\"])\n",
    "\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=config[\"training\"][\"n_folds\"], shuffle=True,\n",
    "                        random_state=config[\"general\"][\"seed\"])\n",
    "for fold, (t_idx, v_idx) in enumerate(kfold.split(train, train[\"target\"])):\n",
    "    train.loc[v_idx, \"kfold\"] = fold\n",
    "\n",
    "\n",
    "    \n",
    "train.to_csv(os.path.join(DATA_ROOT, \"train\", \"train_folds.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations\n",
    "from albumentations import *\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "transforms_train = albumentations.Compose([\n",
    "    Resize(*config[\"preprocessing\"][\"img_size\"]),\n",
    "    Normalize(\n",
    "         mean=[0.485, 0.456, 0.406],\n",
    "         std=[0.229, 0.224, 0.225],\n",
    "     ),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "transforms_val = albumentations.Compose([\n",
    "    Resize(*config[\"preprocessing\"][\"img_size\"]),\n",
    "    Normalize(\n",
    "         mean=[0.485, 0.456, 0.406],\n",
    "         std=[0.229, 0.224, 0.225],\n",
    "     ),\n",
    "    ToTensorV2()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    \n",
    "    \n",
    "def focal_loss(input, target, focus=2.0, raw=False):\n",
    "\n",
    "    if raw:\n",
    "        input = torch.sigmoid(input)\n",
    "\n",
    "    eps = 1e-7\n",
    "\n",
    "    prob_true = input * target + (1 - input) * (1 - target)\n",
    "    prob_true = torch.clamp(prob_true, eps, 1-eps)\n",
    "    modulating_factor = (1.0 - prob_true).pow(focus)\n",
    "\n",
    "    return (-modulating_factor * prob_true.log()).mean()\n",
    "\n",
    "\n",
    "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss\n",
    "\n",
    "\n",
    "class AngularPenaltySMLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, in_features, out_features, loss_type=\"cosface\", eps=1e-7, s=None, m=None):\n",
    "        '''\n",
    "        Angular Penalty Softmax Loss\n",
    "        Three 'loss_types' available: ['arcface', 'sphereface', 'cosface']\n",
    "        These losses are described in the following papers: \n",
    "        \n",
    "        ArcFace: https://arxiv.org/abs/1801.07698\n",
    "        SphereFace: https://arxiv.org/abs/1704.08063\n",
    "        CosFace/Ad Margin: https://arxiv.org/abs/1801.05599\n",
    "        '''\n",
    "        super(AngularPenaltySMLoss, self).__init__()\n",
    "        loss_type = loss_type.lower()\n",
    "        assert loss_type in  ['arcface', 'sphereface', 'cosface']\n",
    "        if loss_type == 'arcface':\n",
    "            self.s = 64.0 if not s else s\n",
    "            self.m = 0.5 if not m else m\n",
    "        if loss_type == 'sphereface':\n",
    "            self.s = 64.0 if not s else s\n",
    "            self.m = 1.35 if not m else m\n",
    "        if loss_type == 'cosface':\n",
    "            self.s = 30.0 if not s else s\n",
    "            self.m = 0.4 if not m else m\n",
    "        self.loss_type = loss_type\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.fc = nn.Linear(in_features, out_features, bias=False)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x, labels):\n",
    "        '''\n",
    "        input shape (N, in_features)\n",
    "        '''\n",
    "        assert len(x) == len(labels)\n",
    "        assert torch.min(labels) >= 0\n",
    "        assert torch.max(labels) < self.out_features\n",
    "        \n",
    "        for W in self.fc.parameters():\n",
    "            W = F.normalize(W, p=2, dim=1)\n",
    "\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "#         print(x.shape)\n",
    "        wf = self.fc(x)\n",
    "        if self.loss_type == 'cosface':\n",
    "            numerator = self.s * (torch.diagonal(wf.transpose(0, 1)[labels]) - self.m)\n",
    "        if self.loss_type == 'arcface':\n",
    "            numerator = self.s * torch.cos(torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1.+self.eps, 1-self.eps)) + self.m)\n",
    "        if self.loss_type == 'sphereface':\n",
    "            numerator = self.s * torch.cos(self.m * torch.acos(torch.clamp(torch.diagonal(wf.transpose(0, 1)[labels]), -1.+self.eps, 1-self.eps)))\n",
    "        excl = torch.cat([torch.cat((wf[i, :y], wf[i, y+1:])).unsqueeze(0) for i, y in enumerate(labels)], dim=0)\n",
    "        denominator = torch.exp(numerator) + torch.sum(torch.exp(self.s * excl), dim=1)\n",
    "        L = numerator - torch.log(denominator)\n",
    "        return -torch.mean(L), wf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = config[\"training\"][\"n_epochs\"]\n",
    "\n",
    "criterion_aam = None\n",
    "if config[\"training\"][\"loss\"] == \"FOCAL\":\n",
    "    criterion = focal_loss\n",
    "elif config[\"training\"][\"loss\"] == \"AAM\":\n",
    "    criterion = \"AAM\"\n",
    "    criterion_aam = AngularPenaltySMLoss\n",
    "elif config[\"training\"][\"loss\"] == \"L1\":\n",
    "    criterion = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = None\n",
    "if config[\"training\"][\"finetune\"]:\n",
    "    model_names = [name for name in os.listdir(config['training']['models_dir']) if name.find(\"best_model_fold\") != -1]\n",
    "    model_names = sorted(model_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10.1708:   0%|          | 3/669 [00:06<24:04,  2.17s/it]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-18a9d5ecbdb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m                                             \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                                             \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                                             task_type=config[\"general\"][\"task_type\"])\n\u001b[0m\u001b[1;32m     53\u001b[0m     \u001b[0msamples2preds_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples2preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0msamples2trues_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples2trues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/paniquex/samsung_2tb/IDAO_2021_oski/src/pipeline_utils.py\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(EPOCHS, model, train_dataloader, val_dataloader, DEVICE, criterion, optimizer, config, scheduler, fold, task_type)\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mimg_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"img\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m             \u001b[0moutput_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# class_) # predictions, mixuped classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"AAM\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "try:\n",
    "    shutil.rmtree(config[\"general\"][\"out_path\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    os.mkdir(config[\"general\"][\"out_path\"])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "if config[\"general\"][\"task_type\"] == \"regression\":\n",
    "    config[\"general\"][\"classes_num\"] = 1\n",
    "samples2preds_all = {}\n",
    "samples2trues_all = {}\n",
    "for i in range(config[\"training\"][\"n_folds\"]):\n",
    "    model_name = config[\"general\"][\"model_name\"]\n",
    "    model = None\n",
    "    model = ENCODER_PARAMS[model_name][\"init_op\"]()\n",
    "    model = Wrapper(model, feat_module=None, classes_num=config[\"general\"][\"classes_num\"],\n",
    "                    model_name=model_name,\n",
    "                    spec_augmenter=None, \n",
    "                    mixup_module=None,\n",
    "                    task_type=config[\"general\"][\"task_type\"],\n",
    "                    activation_func=config[\"training\"][\"activation_func\"],\n",
    "                    criterion_aam=criterion_aam)\n",
    "    model.to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config[\"training\"][\"lr\"])\n",
    "#         optimizer = Ranger(model.parameters(),\n",
    "#                lr=config[\"training\"][\"lr\"],\n",
    "#                betas=(.90, 0.999), k=4)\n",
    "    train_dataset = SimpleDataset(df=train[train[\"kfold\"] != fold], mode=\"train\",\n",
    "                                  transform=transforms_train, classes_num=config[\"general\"][\"classes_num\"],\n",
    "                                  task_type=config[\"general\"][\"task_type\"])\n",
    "\n",
    "    val_dataset = SimpleDataset(df=train[train[\"kfold\"] == fold], mode=\"val\",\n",
    "                                transform=transforms_val, classes_num=config[\"general\"][\"classes_num\"],\n",
    "                                task_type=config[\"general\"][\"task_type\"])\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                  **config[\"training\"][\"dataloader\"])\n",
    "    val_dataloader = DataLoader(val_dataset,\n",
    "                                **config[\"validation\"][\"dataloader\"])\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
    "                                                       T_max=(config[\"training\"][\"n_epochs\"] - config[\"training\"][\"n_epochs_flat\"])  * len(train_dataloader),\n",
    "                                                       eta_min=1e-8)    \n",
    "    samples2preds, samples2trues = training(EPOCHS=EPOCHS, model=model,\n",
    "                                            train_dataloader=train_dataloader, \n",
    "                                            val_dataloader=val_dataloader,\n",
    "                                            DEVICE=DEVICE, criterion=criterion,\n",
    "                                            optimizer=optimizer, scheduler=scheduler,\n",
    "                                            config=config, fold=i,\n",
    "                                            task_type=config[\"general\"][\"task_type\"])\n",
    "    samples2preds_all.update(samples2preds)\n",
    "    samples2trues_all.update(samples2trues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10714"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(train[\"kfold\"] != fold)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
